{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1vr2srg8O8OJ2bCMgJfZGg6wB4Re_QkTh","timestamp":1684126582116},{"file_id":"13HuM7NxfQlr-9Mlw8dnfcmOCxxSB2vEi","timestamp":1684013633751},{"file_id":"1jBVNJgyUai44WrRPH6LxfGetOlCBW1m7","timestamp":1683114211152}],"machine_shape":"hm","collapsed_sections":["pbfn4UwOTQgc","g7Dw1uWBGwVU","14QNzEn0RsMS","5-n2IB-nTBaB"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["## run every time"],"metadata":{"id":"pbfn4UwOTQgc"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","os.chdir(\"/content/drive/MyDrive/ml4nlp2/final_folder\")\n","!ls\n","\n","from  IPython. display  import  clear_output\n","import warnings\n","warnings. filterwarnings('ignore')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KBekCOU9dXw_","executionInfo":{"status":"ok","timestamp":1684053252956,"user_tz":-120,"elapsed":2903,"user":{"displayName":"Upasana Chakraborty","userId":"16670799179179629238"}},"outputId":"b81e2c55-5ba5-4cfb-f07f-506bb2952a36"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","baselines\t\t\t   large_train_data_task1.csv\n","converted_og\t\t\t   models_zoo.xlsx\n","data_process_code\t\t   og_data\n","dev_data_task1.csv\t\t   optuna_models\n","dev_data_task1_en.csv\t\t   output_hard1.json\n","dev_data_task1_es.csv\t\t   output_soft1.json\n","dev_data_task2.csv\t\t   rohit_final_code\n","dev_data_task2_en.csv\t\t   train_data_task1.csv\n","dev_data_task2_es.csv\t\t   train_data_task1_en.csv\n","dev_data_task3.csv\t\t   train_data_task1_es.csv\n","dev_data_task3_en.csv\t\t   train_data_task2.csv\n","dev_data_task3_es.csv\t\t   train_data_task2_en.csv\n","exist2023evaluation.py\t\t   train_data_task2_es.csv\n","EXIST2023_test_clean.json\t   train_data_task3.csv\n","golds\t\t\t\t   train_data_task3_en.csv\n","good_paper_distilled_learning.pdf  train_data_task3_es.csv\n","hp_optune\t\t\t   translated_og\n"]}]},{"cell_type":"code","source":["#set device\n","import torch\n","# device = torch.device(\"cpu\")\n","device  =  torch. device('cuda:0'  if  torch. cuda. is_available() else  'cpu')\n","print(f\"device is {device}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JW4tr577CiCJ","executionInfo":{"status":"ok","timestamp":1684053254102,"user_tz":-120,"elapsed":1153,"user":{"displayName":"Upasana Chakraborty","userId":"16670799179179629238"}},"outputId":"f7201b69-217a-4070-d6f1-c117aad6ddb7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["device is cuda:0\n"]}]},{"cell_type":"code","source":["!pip install optuna\n","!pip install datasets\n","!pip install transformers==4.28.0\n","!pip install sentencepiece==0.1.98\n","!pip install pysentimiento\n","# !pip install --upgrade pip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z688y8k5dsTp","executionInfo":{"status":"ok","timestamp":1684053281206,"user_tz":-120,"elapsed":27108,"user":{"displayName":"Upasana Chakraborty","userId":"16670799179179629238"}},"outputId":"52011a8d-8582-4dfd-d6e9-e73c56f6e590"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: optuna in /usr/local/lib/python3.10/dist-packages (3.1.1)\n","Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (1.10.4)\n","Requirement already satisfied: cmaes>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from optuna) (0.9.1)\n","Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna) (6.7.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (23.1)\n","Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.10)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.65.0)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0)\n","Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (1.2.4)\n","Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.5.0)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (2.0.2)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.2)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.12.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n","Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.6)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.2.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.14)\n","Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.4.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.4)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.14.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n","Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.18.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.2)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers==4.28.0 in /usr/local/lib/python3.10/dist-packages (4.28.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (3.12.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (0.14.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (2.27.1)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (0.13.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (2023.4.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (3.4)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: sentencepiece==0.1.98 in /usr/local/lib/python3.10/dist-packages (0.1.98)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pysentimiento in /usr/local/lib/python3.10/dist-packages (0.7.0)\n","Requirement already satisfied: datasets>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from pysentimiento) (2.12.0)\n","Requirement already satisfied: emoji<2.0.0,>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from pysentimiento) (1.7.0)\n","Requirement already satisfied: spacy<4.0.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from pysentimiento) (3.5.2)\n","Requirement already satisfied: torch==2.0.0 in /usr/local/lib/python3.10/dist-packages (from pysentimiento) (2.0.0+cu118)\n","Requirement already satisfied: transformers>=4.13.0 in /usr/local/lib/python3.10/dist-packages (from pysentimiento) (4.28.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->pysentimiento) (3.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->pysentimiento) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->pysentimiento) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->pysentimiento) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->pysentimiento) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->pysentimiento) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0->pysentimiento) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0->pysentimiento) (16.0.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.13.3->pysentimiento) (1.22.4)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.13.3->pysentimiento) (9.0.0)\n","Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.13.3->pysentimiento) (0.3.6)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=1.13.3->pysentimiento) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.13.3->pysentimiento) (2.27.1)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.13.3->pysentimiento) (4.65.0)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=1.13.3->pysentimiento) (3.2.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets>=1.13.3->pysentimiento) (0.70.14)\n","Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.13.3->pysentimiento) (2023.4.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=1.13.3->pysentimiento) (3.8.4)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.13.3->pysentimiento) (0.14.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets>=1.13.3->pysentimiento) (23.1)\n","Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.13.3->pysentimiento) (0.18.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.13.3->pysentimiento) (6.0)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->pysentimiento) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->pysentimiento) (1.0.4)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->pysentimiento) (1.0.9)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->pysentimiento) (2.0.7)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->pysentimiento) (3.0.8)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->pysentimiento) (8.1.9)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->pysentimiento) (1.1.1)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->pysentimiento) (2.4.6)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->pysentimiento) (2.0.8)\n","Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->pysentimiento) (0.7.0)\n","Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->pysentimiento) (0.10.1)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->pysentimiento) (6.3.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->pysentimiento) (1.10.7)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->pysentimiento) (67.7.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->pysentimiento) (3.3.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.13.0->pysentimiento) (2022.10.31)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.13.0->pysentimiento) (0.13.3)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.13.3->pysentimiento) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.13.3->pysentimiento) (2.0.12)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.13.3->pysentimiento) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.13.3->pysentimiento) (4.0.2)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.13.3->pysentimiento) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.13.3->pysentimiento) (1.3.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.13.3->pysentimiento) (1.3.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.13.3->pysentimiento) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.13.3->pysentimiento) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.13.3->pysentimiento) (3.4)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<4.0.0,>=3.5.0->pysentimiento) (0.7.9)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<4.0.0,>=3.5.0->pysentimiento) (0.0.4)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy<4.0.0,>=3.5.0->pysentimiento) (8.1.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.0->pysentimiento) (2.1.2)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=1.13.3->pysentimiento) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=1.13.3->pysentimiento) (2022.7.1)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.0->pysentimiento) (1.3.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.13.3->pysentimiento) (1.16.0)\n"]}]},{"cell_type":"code","source":["import optuna \n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer  \n","import os.path\n","from os import path\n","\n","# Create function for printing \n","def print_custom(text):\n","    print('\\n')\n","    print(text)\n","    print('-'*100)"],"metadata":{"id":"xjK6YC6hdqWH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import random\n","import  matplotlib. pyplot  as  plt\n","from tqdm import tqdm\n","\n","import torch\n","import torch.optim as optim\n","import  torch. nn. functional  as  F\n","\n","from  IPython. display  import  clear_output\n","import warnings\n","warnings. filterwarnings('ignore')\n","\n","#setting device agnostic version\n","# device = \"cpu\"\n","device  =  torch. device('cuda:0'  if  torch. cuda. is_available() else  'cpu')\n","print(f\"computation will run on {device} now\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bvxFZT2keJBp","executionInfo":{"status":"ok","timestamp":1684053285250,"user_tz":-120,"elapsed":28,"user":{"displayName":"Upasana Chakraborty","userId":"16670799179179629238"}},"outputId":"bd7348d4-946e-40d5-e915-6c12ab396803"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["computation will run on cuda:0 now\n"]}]},{"cell_type":"code","source":["import re\n","def simple_preprocess(text):\n","  \"\"\"\n","  pass the tweet data as a series. do not use apply function\n","  only preprocesses for replacing @USER and URLS\n","  \"\"\"\n","  # print(\"i am preprocessing\")\n","  URL_RE = re.compile(r\"https?:\\/\\/[\\w\\.\\/\\?\\=\\d&#%_:/-]+\")\n","  HANDLE_RE = re.compile(r\"@\\w+\")\n","  tweets = []\n","  for t in text:\n","    t = HANDLE_RE.sub(\"@USER\", t)\n","    t = URL_RE.sub(\"HTTPURL\", t)\n","    tweets.append(t)\n","  return tweets"],"metadata":{"id":"0n8Yv474elyz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## construct optuna - all tasks hard"],"metadata":{"id":"IIulN2ntovt8"}},{"cell_type":"markdown","source":["### construct optuna - task1 hard"],"metadata":{"id":"g7Dw1uWBGwVU"}},{"cell_type":"code","source":["#relevant functions\n","import re\n","# Create function for printing \n","def print_custom(text):\n","    print('\\n')\n","    print(text)\n","    print('-'*100)\n","    \n","def simple_preprocess(text):\n","  \"\"\"\n","  pass the tweet data as a series. do not use apply function\n","  only preprocesses for replacing @USER and URLS\n","  \"\"\"\n","  # print(\"i am preprocessing\")\n","  URL_RE = re.compile(r\"https?:\\/\\/[\\w\\.\\/\\?\\=\\d&#%_:/-]+\")\n","  HANDLE_RE = re.compile(r\"@\\w+\")\n","  tweets = []\n","  for t in text:\n","    t = HANDLE_RE.sub(\"@USER\", t)\n","    t = URL_RE.sub(\"HTTPURL\", t)\n","    tweets.append(t)\n","  return tweets\n","\n","#instantiate label encoders\n","from sklearn.preprocessing import LabelEncoder\n","task1_encoder = LabelEncoder()\n","task2_encoder = LabelEncoder()\n","task3_encoder = LabelEncoder() \n","\n","def task1_hard_encode(df):\n","    task1_encoder.fit(all_task1_hard_labels)\n","    df['hard_label'] = task1_encoder.transform(df['hard_label'])\n","    return df\n","\n","def task1_hard_decode(df):\n","    df[\"hard_label\"] = task1_encoder.inverse_transform(df[\"hard_label\"])\n","    return df\n","\n","def task2_hard_encode(df):\n","    task2_encoder.fit(all_task2_hard_labels)\n","    df['hard_label'] = task1_encoder.transform(df['hard_label'])\n","    return df\n","\n","def task2_hard_decode(df):\n","    df[\"hard_label\"] = task2_encoder.inverse_transform(df[\"hard_label\"])\n","    return df\n","\n","\n","import optuna \n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer  \n","import os.path\n","from os import path\n","\n","from transformers import XLMRobertaForSequenceClassification, XLMRobertaTokenizer, TrainingArguments, Trainer\n","from datasets import Dataset\n","\n","\n","#load given data\n","import pandas as pd\n","og_train1 = pd.read_csv(\"train_data_task1.csv\")\n","og_dev1 = pd.read_csv(\"dev_data_task1.csv\")\n","\n","all_task1_hard_labels = pd.concat([og_train1[\"hard_label\"],og_dev1[\"hard_label\"]])\n","\n","train1_df = task1_hard_encode(og_train1)\n","# print(train1_df.columns)\n","train1_df = train1_df[[\"tweet\",\"hard_label\"]].dropna()\n","train1_df = train1_df[train1_df['hard_label'] != 2]\n","train1_df[\"tweet\"] = simple_preprocess(train1_df[\"tweet\"])\n","\n","test1_df = task1_hard_encode(og_dev1)\n","test1_df = test1_df[[\"tweet\",\"hard_label\"]].dropna()\n","test1_df = test1_df[test1_df['hard_label'] != 2]\n","test1_df[\"tweet\"] = simple_preprocess(test1_df[\"tweet\"])\n","\n","print(\"train1\",train1_df.shape)\n","print(train1_df.head)\n","print(\"test1\",test1_df.shape)\n","print(test1_df.head)\n","\n","\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","# Combine train1df and test1df into a single dataframe\n","combined_df = pd.concat([train1_df, test1_df], ignore_index=True)\n","\n","# Shuffle the combined dataframe\n","combined_df_shuffled = combined_df.sample(frac=1, random_state=42)\n","\n","# Split the shuffled dataframe into train, validation, and test dataframes with an 80-10-10 split\n","train_df, temp_df = train_test_split(combined_df_shuffled, test_size=0.2, random_state=42)\n","val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n","\n","# Reset the indices of the dataframes\n","train_df = train_df.reset_index(drop=True)\n","val_df = val_df.reset_index(drop=True)\n","test_df = test_df.reset_index(drop=True)\n","\n","\n","# Load the tokenizer and model\n","model_name = \"sdadas/xlm-roberta-large-twitter\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2,ignore_mismatched_sizes=True)\n","\n","# Set parameters\n","LR_MIN = 1e-6\n","LR_CEIL = 0.001\n","WD_MIN = 4e-5\n","WD_CEIL = 0.01\n","MIN_EPOCHS = 2\n","MAX_EPOCHS = 5\n","PER_DEVICE_EVAL_BATCH = 16\n","PER_DEVICE_TRAIN_BATCH = 8\n","NUM_TRIALS = 1\n","SAVE_DIR = '/content/drive/MyDrive/ml4nlp2/final_folder/optuna_models/'\n","NAME_OF_MODEL = 'sexist_tweet_predictor_task1_hard'\n","MAX_LENGTH = 128\n","\n","def convert_to_dataset(df):\n","    df = {\"text\": df['tweet'].tolist(), \"label\": df[\"hard_label\"].tolist()}\n","    dataset = Dataset.from_dict(df)\n","    return dataset\n","\n","# Convert dataframe to dataset\n","train_dataset = convert_to_dataset(train_df)\n","val_dataset = convert_to_dataset(val_df)\n","test_dataset = convert_to_dataset(test_df)\n","\n","\n","# Create the datasets\n","train_encodings = tokenizer(train_dataset[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\n","train_dataset = Dataset.from_dict({\"input_ids\": train_encodings[\"input_ids\"], \"attention_mask\": train_encodings[\"attention_mask\"], \"label\": train_dataset[\"label\"]})\n","\n","val_encodings = tokenizer(val_dataset[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\n","val_dataset = Dataset.from_dict({\"input_ids\": val_encodings[\"input_ids\"], \"attention_mask\": val_encodings[\"attention_mask\"], \"label\": val_dataset[\"label\"]})\n","\n","\n","test_encodings = tokenizer(test_dataset[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\n","test_dataset = Dataset.from_dict({\"input_ids\": test_encodings[\"input_ids\"], \"attention_mask\": test_encodings[\"attention_mask\"], \"label\": test_dataset[\"label\"]})\n","\n","\n","\n","def objective(trial):\n","    training_args = TrainingArguments(\n","        output_dir=SAVE_DIR,\n","        learning_rate=trial.suggest_loguniform('learning_rate', LR_MIN, LR_CEIL),\n","        weight_decay=trial.suggest_loguniform('weight_decay', WD_MIN, WD_CEIL),\n","        num_train_epochs=trial.suggest_int('num_train_epochs', MIN_EPOCHS, MAX_EPOCHS),\n","        per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH,\n","        per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH,\n","        disable_tqdm=False\n","    )\n","\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_dataset,\n","        eval_dataset=val_dataset\n","    )\n","\n","    result = trainer.train()\n","    return result.training_loss\n","\n","study = optuna.create_study(study_name='hp-search-task1-hard', direction='minimize')\n","study.optimize(objective, n_trials=NUM_TRIALS)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":863},"id":"VWB0tKijolRr","executionInfo":{"status":"ok","timestamp":1684017115294,"user_tz":-120,"elapsed":1760541,"user":{"displayName":"Upasana Chakraborty","userId":"16670799179179629238"}},"outputId":"cfa499af-e3cc-45b2-f54f-9ca71ae97dc6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["train1 (6064, 2)\n","<bound method NDFrame.head of                                                   tweet  hard_label\n","0     @USER Ignora al otro, es un capullo.El problem...           1\n","1     @USER Si comicsgate se parece en algo a gamerg...           0\n","2     @USER Lee sobre Gamergate, y como eso ha cambi...           0\n","4     @USER @USER @USER Entonces como así es el merc...           1\n","5     @USER Aaah sí. Andrew Dobson. El que se dedicó...           0\n","...                                                 ...         ...\n","6915  idk why y’all bitches think having half your a...           1\n","6916  This has been a part of an experiment with @US...           1\n","6917  \"Take me already\" \"Not yet. You gotta be ready...           1\n","6918    @USER why do you look like a whore? /lh HTTPURL           1\n","6919  ik when mandy says “you look like a whore” i l...           1\n","\n","[6064 rows x 2 columns]>\n","test1 (934, 2)\n","<bound method NDFrame.head of                                                   tweet  hard_label\n","1     @USER @USER No me acuerdo de los detalles de G...           1\n","2     @USER lo digo cada pocos dias y lo repito: TOD...           0\n","3     Also mientras les decia eso la señalaba y deci...           1\n","4     And all people killed,  attacked, harassed by ...           0\n","5     On this #WorldPressFreedomDay I’m thinking of ...           0\n","...                                                 ...         ...\n","1033  @USER “Don’t wear a black bra with a white ves...           1\n","1034  \" get changed , you look like a prostitute . \"...           1\n","1035  made this top and my mom gave me the “you look...           1\n","1036  @USER I haven't seen anything that makes you l...           1\n","1037  @USER You look like a whore in ur new picture....           1\n","\n","[934 rows x 2 columns]>\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at sdadas/xlm-roberta-large-twitter were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at sdadas/xlm-roberta-large-twitter and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[32m[I 2023-05-13 22:02:41,792]\u001b[0m A new study created in memory with name: hp-search-task1-hard\u001b[0m\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='2100' max='2100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2100/2100 29:02, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.493300</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.381800</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.275600</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.160800</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2023-05-13 22:31:54,388]\u001b[0m Trial 0 finished with value: 0.3187674554189046 and parameters: {'learning_rate': 3.395067563962819e-05, 'weight_decay': 0.004891290652279793, 'num_train_epochs': 3}. Best is trial 0 with value: 0.3187674554189046.\u001b[0m\n"]}]},{"cell_type":"code","source":["print_custom('Finding study best parameters')\n","best_lr = float(study.best_params['learning_rate'])\n","best_weight_decay = float(study.best_params['weight_decay'])\n","best_epoch = int(study.best_params['num_train_epochs'])\n","print_custom('Extract best study params')\n","\n","print(f'The best learning rate is: {best_lr}')\n","print(f'The best weight decay is: {best_weight_decay}')\n","print(f'The best epoch is : {best_epoch}')\n","\n","print_custom('Create dictionary of the best hyperparameters')\n","best_hp_dict = {\n","    'best_learning_rate' : best_lr,\n","    'best_weight_decay': best_weight_decay,\n","    'best_epoch': best_epoch\n","}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XAMs-P9oJ6h5","executionInfo":{"status":"ok","timestamp":1684017653351,"user_tz":-120,"elapsed":428,"user":{"displayName":"Upasana Chakraborty","userId":"16670799179179629238"}},"outputId":"45682ba4-6126-4c8c-84e0-9f7b7c99684b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","Finding study best parameters\n","----------------------------------------------------------------------------------------------------\n","\n","\n","Extract best study params\n","----------------------------------------------------------------------------------------------------\n","The best learning rate is: 3.395067563962819e-05\n","The best weight decay is: 0.004891290652279793\n","The best epoch is : 3\n","\n","\n","Create dictionary of the best hyperparameters\n","----------------------------------------------------------------------------------------------------\n"]}]},{"cell_type":"markdown","source":["### task 2 hard"],"metadata":{"id":"14QNzEn0RsMS"}},{"cell_type":"code","source":["#relevant functions\n","import re\n","# Create function for printing \n","def print_custom(text):\n","    print('\\n')\n","    print(text)\n","    print('-'*100)\n","    \n","def simple_preprocess(text):\n","  \"\"\"\n","  pass the tweet data as a series. do not use apply function\n","  only preprocesses for replacing @USER and URLS\n","  \"\"\"\n","  # print(\"i am preprocessing\")\n","  URL_RE = re.compile(r\"https?:\\/\\/[\\w\\.\\/\\?\\=\\d&#%_:/-]+\")\n","  HANDLE_RE = re.compile(r\"@\\w+\")\n","  tweets = []\n","  for t in text:\n","    t = HANDLE_RE.sub(\"@USER\", t)\n","    t = URL_RE.sub(\"HTTPURL\", t)\n","    tweets.append(t)\n","  return tweets\n","\n","def tweet_preprocessor_spanish(text):\n","    \"\"\"\n","    imported from pysentimiento \n","    does the following: \n","    1. changes to @usario\n","    2. shortens characters repetitions to 3 (can be changed to two or more)\n","    3. converts emojis to text type of need\n","    4. normalises laughter expressions\n","    5. handles hashtags -- removes the #\n","    \"\"\"\n","    from pysentimiento.preprocessing import preprocess_tweet\n","    return preprocess_tweet(text, lang=\"es\", shorten=3)\n","\n","def tweet_preprocessor_english(text):\n","    \"\"\"\n","    imported from pysentimiento \n","    does the following: \n","    1. changes to @usario\n","    2. shortens characters repetitions to 3 (can be changed to two or more)\n","    3. converts emojis to text type of need\n","    4. normalises laughter expressions\n","    5. handles hashtags -- removes the #\n","    \"\"\"\n","    from pysentimiento.preprocessing import preprocess_tweet\n","    return preprocess_tweet(text, lang=\"en\", shorten=3)\n","\n","#instantiate label encoders\n","from sklearn.preprocessing import LabelEncoder\n","task_encoder = LabelEncoder()\n","\n","def task_hard_encode(df):\n","    task_encoder.fit(all_task_hard_labels)\n","    df['hard_label'] = task_encoder.transform(df['hard_label'])\n","    return df\n","\n","def task_hard_decode(df):\n","    df[\"hard_label\"] = task_encoder.inverse_transform(df[\"hard_label\"])\n","    return df\n","\n","\n","import optuna \n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer  \n","import os.path\n","from os import path\n","\n","from transformers import XLMRobertaForSequenceClassification, XLMRobertaTokenizer, TrainingArguments, Trainer\n","from datasets import Dataset\n","\n","\n","#load given data\n","import pandas as pd\n","og_train1 = pd.read_csv(\"train_data_task2.csv\")\n","og_dev1 = pd.read_csv(\"dev_data_task2.csv\")\n","\n","all_task_hard_labels = pd.concat([og_train1[\"hard_label\"],og_dev1[\"hard_label\"]])\n","\n","train1_df = task_hard_encode(og_train1)\n","# print(train1_df.columns)\n","train1_df = train1_df[[\"tweet\",\"hard_label\"]].dropna()\n","train1_df = train1_df[train1_df['hard_label'] != 4] #removing nans\n","train1_df[\"tweet\"] = simple_preprocess(train1_df[\"tweet\"])\n","\n","test1_df = task_hard_encode(og_dev1)\n","test1_df = test1_df[[\"tweet\",\"hard_label\"]].dropna()\n","test1_df = test1_df[test1_df['hard_label'] != 4] #removing nans\n","test1_df[\"tweet\"] = simple_preprocess(test1_df[\"tweet\"])\n","\n","print(\"train1\",train1_df.shape)\n","print(train1_df.head)\n","print(\"test1\",test1_df.shape)\n","print(test1_df.head)\n","\n","\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","# Combine train1df and test1df into a single dataframe\n","combined_df = pd.concat([train1_df, test1_df], ignore_index=True)\n","\n","# Shuffle the combined dataframe\n","combined_df_shuffled = combined_df.sample(frac=1, random_state=42)\n","\n","# Split the shuffled dataframe into train, validation, and test dataframes with an 80-10-10 split\n","train_df, temp_df = train_test_split(combined_df_shuffled, test_size=0.2, random_state=42)\n","val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n","\n","# Reset the indices of the dataframes\n","train_df = train_df.reset_index(drop=True)\n","val_df = val_df.reset_index(drop=True)\n","test_df = test_df.reset_index(drop=True)\n","\n","\n","# Load the tokenizer and model\n","model_name = \"sdadas/xlm-roberta-large-twitter\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=4,ignore_mismatched_sizes=True)\n","\n","# Set parameters\n","LR_MIN = 1e-6\n","LR_CEIL = 0.001\n","WD_MIN = 4e-5\n","WD_CEIL = 0.01\n","MIN_EPOCHS = 2\n","MAX_EPOCHS = 5\n","PER_DEVICE_EVAL_BATCH = 16\n","PER_DEVICE_TRAIN_BATCH = 8\n","NUM_TRIALS = 1\n","SAVE_DIR = '/content/drive/MyDrive/ml4nlp2/final_folder/optuna_models/'\n","NAME_OF_MODEL = 'sexist_tweet_predictor_task1_hard'\n","MAX_LENGTH = 128\n","\n","def convert_to_dataset(df):\n","    df = {\"text\": df['tweet'].tolist(), \"label\": df[\"hard_label\"].tolist()}\n","    dataset = Dataset.from_dict(df)\n","    return dataset\n","\n","# Convert dataframe to dataset\n","train_dataset = convert_to_dataset(train_df)\n","val_dataset = convert_to_dataset(val_df)\n","test_dataset = convert_to_dataset(test_df)\n","\n","\n","# Create the datasets\n","train_encodings = tokenizer(train_dataset[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\n","train_dataset = Dataset.from_dict({\"input_ids\": train_encodings[\"input_ids\"], \"attention_mask\": train_encodings[\"attention_mask\"], \"label\": train_dataset[\"label\"]})\n","\n","val_encodings = tokenizer(val_dataset[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\n","val_dataset = Dataset.from_dict({\"input_ids\": val_encodings[\"input_ids\"], \"attention_mask\": val_encodings[\"attention_mask\"], \"label\": val_dataset[\"label\"]})\n","\n","\n","test_encodings = tokenizer(test_dataset[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\n","test_dataset = Dataset.from_dict({\"input_ids\": test_encodings[\"input_ids\"], \"attention_mask\": test_encodings[\"attention_mask\"], \"label\": test_dataset[\"label\"]})\n","\n","\n","\n","def objective(trial):\n","    training_args = TrainingArguments(\n","        output_dir=SAVE_DIR,\n","        learning_rate=trial.suggest_loguniform('learning_rate', LR_MIN, LR_CEIL),\n","        weight_decay=trial.suggest_loguniform('weight_decay', WD_MIN, WD_CEIL),\n","        num_train_epochs=trial.suggest_int('num_train_epochs', MIN_EPOCHS, MAX_EPOCHS),\n","        per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH,\n","        per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH,\n","        disable_tqdm=False\n","    )\n","\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_dataset,\n","        eval_dataset=val_dataset\n","    )\n","\n","    result = trainer.train()\n","    return result.training_loss\n","\n","study = optuna.create_study(study_name='hp-search-task1-hard', direction='minimize')\n","study.optimize(objective, n_trials=NUM_TRIALS)\n","\n","print_custom('Finding study best parameters')\n","best_lr = float(study.best_params['learning_rate'])\n","best_weight_decay = float(study.best_params['weight_decay'])\n","best_epoch = int(study.best_params['num_train_epochs'])\n","print_custom('Extract best study params')\n","\n","print(f'The best learning rate is: {best_lr}')\n","print(f'The best weight decay is: {best_weight_decay}')\n","print(f'The best epoch is : {best_epoch}')\n","\n","print_custom('Create dictionary of the best hyperparameters')\n","best_hp_dict = {\n","    'best_learning_rate' : best_lr,\n","    'best_weight_decay': best_weight_decay,\n","    'best_epoch': best_epoch\n","}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"wMsLHZGkZeIW","executionInfo":{"status":"ok","timestamp":1684020298763,"user_tz":-120,"elapsed":2147298,"user":{"displayName":"Upasana Chakraborty","userId":"16670799179179629238"}},"outputId":"df7bfa54-5d7b-40e7-d8d8-1a4eb47321dd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["train1 (5496, 2)\n","<bound method NDFrame.head of                                                   tweet  hard_label\n","0     @USER Ignora al otro, es un capullo.El problem...           3\n","1     @USER Si comicsgate se parece en algo a gamerg...           2\n","2     @USER Lee sobre Gamergate, y como eso ha cambi...           2\n","5     @USER Aaah sí. Andrew Dobson. El que se dedicó...           2\n","7     @USER Esta gringa sigue llorando por el gamerg...           0\n","...                                                 ...         ...\n","6913  @USER Ma'am if I say that you look like a whor...           0\n","6915  idk why y’all bitches think having half your a...           0\n","6916  This has been a part of an experiment with @US...           1\n","6917  \"Take me already\" \"Not yet. You gotta be ready...           0\n","6918    @USER why do you look like a whore? /lh HTTPURL           0\n","\n","[5496 rows x 2 columns]>\n","test1 (841, 2)\n","<bound method NDFrame.head of                                                   tweet  hard_label\n","1     @USER @USER No me acuerdo de los detalles de G...           1\n","2     @USER lo digo cada pocos dias y lo repito: TOD...           2\n","3     Also mientras les decia eso la señalaba y deci...           3\n","4     And all people killed,  attacked, harassed by ...           2\n","5     On this #WorldPressFreedomDay I’m thinking of ...           2\n","...                                                 ...         ...\n","1033  @USER “Don’t wear a black bra with a white ves...           0\n","1034  \" get changed , you look like a prostitute . \"...           3\n","1035  made this top and my mom gave me the “you look...           3\n","1036  @USER I haven't seen anything that makes you l...           0\n","1037  @USER You look like a whore in ur new picture....           0\n","\n","[841 rows x 2 columns]>\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at sdadas/xlm-roberta-large-twitter were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at sdadas/xlm-roberta-large-twitter and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[32m[I 2023-05-13 22:49:21,057]\u001b[0m A new study created in memory with name: hp-search-task1-hard\u001b[0m\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='2536' max='2536' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2536/2536 35:25, Epoch 4/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>1.133600</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>1.103600</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>1.075400</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>1.077800</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>1.062900</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2023-05-13 23:24:57,734]\u001b[0m Trial 0 finished with value: 1.089178811864522 and parameters: {'learning_rate': 0.0004005073373276802, 'weight_decay': 0.0053619187660176775, 'num_train_epochs': 4}. Best is trial 0 with value: 1.089178811864522.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Finding study best parameters\n","----------------------------------------------------------------------------------------------------\n","\n","\n","Extract best study params\n","----------------------------------------------------------------------------------------------------\n","The best learning rate is: 0.0004005073373276802\n","The best weight decay is: 0.0053619187660176775\n","The best epoch is : 4\n","\n","\n","Create dictionary of the best hyperparameters\n","----------------------------------------------------------------------------------------------------\n"]}]},{"cell_type":"markdown","source":["### task 3 hard"],"metadata":{"id":"5-n2IB-nTBaB"}},{"cell_type":"code","source":["#relevant functions\n","import re\n","# Create function for printing \n","def print_custom(text):\n","    print('\\n')\n","    print(text)\n","    print('-'*100)\n","    \n","def simple_preprocess(text):\n","  \"\"\"\n","  pass the tweet data as a series. do not use apply function\n","  only preprocesses for replacing @USER and URLS\n","  \"\"\"\n","  # print(\"i am preprocessing\")\n","  URL_RE = re.compile(r\"https?:\\/\\/[\\w\\.\\/\\?\\=\\d&#%_:/-]+\")\n","  HANDLE_RE = re.compile(r\"@\\w+\")\n","  tweets = []\n","  for t in text:\n","    t = HANDLE_RE.sub(\"@USER\", t)\n","    t = URL_RE.sub(\"HTTPURL\", t)\n","    tweets.append(t)\n","  return tweets\n","\n","def tweet_preprocessor_spanish(text):\n","    \"\"\"\n","    imported from pysentimiento \n","    does the following: \n","    1. changes to @usario\n","    2. shortens characters repetitions to 3 (can be changed to two or more)\n","    3. converts emojis to text type of need\n","    4. normalises laughter expressions\n","    5. handles hashtags -- removes the #\n","    \"\"\"\n","    from pysentimiento.preprocessing import preprocess_tweet\n","    return preprocess_tweet(text, lang=\"es\", shorten=3)\n","\n","def tweet_preprocessor_english(text):\n","    \"\"\"\n","    imported from pysentimiento \n","    does the following: \n","    1. changes to @usario\n","    2. shortens characters repetitions to 3 (can be changed to two or more)\n","    3. converts emojis to text type of need\n","    4. normalises laughter expressions\n","    5. handles hashtags -- removes the #\n","    \"\"\"\n","    from pysentimiento.preprocessing import preprocess_tweet\n","    return preprocess_tweet(text, lang=\"en\", shorten=3)\n","\n","from sklearn.preprocessing import MultiLabelBinarizer\n","\n","# Instantiate multi-label encoder\n","task_encoder = MultiLabelBinarizer()\n","\n","   \n","def task_hard_encode(df):\n","    import numpy as np\n","    # Create a new DataFrame to store the transformed labels\n","    transformed_df = df.copy()\n","    task_encoder.fit(all_task_hard_labels)\n","    all_labels = []\n","    for i in range(len(df['hard_label'])):\n","        # Apply eval to each element of the label column\n","        all_labels.append(eval(df['hard_label'][i]))\n","    \n","    transformed_labels = task_encoder.transform(all_labels)\n","    # print(pd.DataFrame(transformed_labels))\n","    # transformed_df[\"hard_label\"] = pd.DataFrame(transformed_labels)\n","    transformed_labels = pd.Series([x.tolist() for x in transformed_labels])\n","    transformed_df[\"hard_label\"] = transformed_labels\n","    return transformed_df\n","\n","def task_hard_decode(df):\n","    # Inverse transform the binary-encoded vectors back to original labels\n","    import numpy as np\n","    decoded_df = df.copy()\n","    decode_labels = np.array(decoded_df[\"hard_label\"].tolist())\n","\n","    # Inverse transform the label column using the provided task_encoder\n","    decoded_labels = task_encoder.inverse_transform(decode_labels)\n","    \n","    # Assign the decoded labels back to the DataFrame\n","    decoded_df[\"hard_labels\"] = decoded_labels\n","\n","    return decoded_df\n","\n","import optuna \n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer  \n","import os.path\n","from os import path\n","\n","from transformers import XLMRobertaForSequenceClassification, XLMRobertaTokenizer, TrainingArguments, Trainer\n","from datasets import Dataset\n","\n","#load given data\n","import pandas as pd\n","og_train1 = pd.read_csv(\"train_data_task3.csv\")\n","og_dev1 = pd.read_csv(\"dev_data_task3.csv\")\n","\n","train1_df = og_train1[[\"tweet\",\"hard_label\"]].dropna().reset_index(drop=True)\n","\n","test1_df = og_dev1[[\"tweet\",\"hard_label\"]].dropna().reset_index(drop=True)\n","\n","all_task_hard_labels = pd.concat([og_train1[\"hard_label\"],og_dev1[\"hard_label\"]]).dropna()\n","all_task_hard_labels = all_task_hard_labels.apply(lambda x: eval(x)).tolist()\n","print(all_task_hard_labels[0])\n","\n","train1_df = task_hard_encode(train1_df)\n","# print(train1_df.columns)\n","train1_df = train1_df[[\"tweet\",\"hard_label\"]].dropna()\n","train1_df[\"tweet\"] = simple_preprocess(train1_df[\"tweet\"])\n","\n","test1_df = task_hard_encode(test1_df)\n","test1_df[\"tweet\"] = simple_preprocess(test1_df[\"tweet\"])\n","\n","print(\"train1\",train1_df.shape)\n","print(train1_df.head)\n","print(\"test1\",test1_df.shape)\n","print(test1_df.head)\n","\n","\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","# Combine train1df and test1df into a single dataframe\n","combined_df = pd.concat([train1_df, test1_df], ignore_index=True)\n","\n","# Shuffle the combined dataframe\n","combined_df_shuffled = combined_df.sample(frac=1, random_state=42)\n","\n","# Split the shuffled dataframe into train, validation, and test dataframes with an 80-10-10 split\n","train_df, temp_df = train_test_split(combined_df_shuffled, test_size=0.2, random_state=42)\n","val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n","\n","# Reset the indices of the dataframes\n","train_df = train_df.reset_index(drop=True)\n","val_df = val_df.reset_index(drop=True)\n","test_df = test_df.reset_index(drop=True)\n","\n","\n","# Load the tokenizer and model\n","model_name = \"sdadas/xlm-roberta-large-twitter\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=6,ignore_mismatched_sizes=True)\n","\n","# Set parameters\n","LR_MIN = 1e-6\n","LR_CEIL = 0.001\n","WD_MIN = 4e-5\n","WD_CEIL = 0.01\n","MIN_EPOCHS = 2\n","MAX_EPOCHS = 5\n","PER_DEVICE_EVAL_BATCH = 16\n","PER_DEVICE_TRAIN_BATCH = 8\n","NUM_TRIALS = 1\n","SAVE_DIR = '/content/drive/MyDrive/ml4nlp2/final_folder/optuna_models/'\n","NAME_OF_MODEL = 'sexist_tweet_predictor_task1_hard'\n","MAX_LENGTH = 128\n","\n","def convert_to_dataset(df):\n","    df = {\"text\": df['tweet'].tolist(), \"label\": df[\"hard_label\"].tolist()}\n","    dataset = Dataset.from_dict(df)\n","    return dataset\n","\n","# Convert dataframe to dataset\n","train_dataset = convert_to_dataset(train_df)\n","val_dataset = convert_to_dataset(val_df)\n","test_dataset = convert_to_dataset(test_df)\n","\n","\n","# Create the datasets\n","train_encodings = tokenizer(train_dataset[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\n","train_dataset = Dataset.from_dict({\"input_ids\": train_encodings[\"input_ids\"], \"attention_mask\": train_encodings[\"attention_mask\"], \"label\": train_dataset[\"label\"]})\n","\n","val_encodings = tokenizer(val_dataset[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\n","val_dataset = Dataset.from_dict({\"input_ids\": val_encodings[\"input_ids\"], \"attention_mask\": val_encodings[\"attention_mask\"], \"label\": val_dataset[\"label\"]})\n","\n","\n","test_encodings = tokenizer(test_dataset[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\n","test_dataset = Dataset.from_dict({\"input_ids\": test_encodings[\"input_ids\"], \"attention_mask\": test_encodings[\"attention_mask\"], \"label\": test_dataset[\"label\"]})\n","\n","\n","\n","def objective(trial):\n","    training_args = TrainingArguments(\n","        output_dir=SAVE_DIR,\n","        learning_rate=trial.suggest_loguniform('learning_rate', LR_MIN, LR_CEIL),\n","        weight_decay=trial.suggest_loguniform('weight_decay', WD_MIN, WD_CEIL),\n","        num_train_epochs=trial.suggest_int('num_train_epochs', MIN_EPOCHS, MAX_EPOCHS),\n","        per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH,\n","        per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH,\n","        disable_tqdm=False\n","    )\n","\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_dataset,\n","        eval_dataset=val_dataset\n","    )\n","\n","    result = trainer.train()\n","    return result.training_loss\n","\n","study = optuna.create_study(study_name='hp-search-task1-hard', direction='minimize')\n","study.optimize(objective, n_trials=NUM_TRIALS)\n","\n","print_custom('Finding study best parameters')\n","best_lr = float(study.best_params['learning_rate'])\n","best_weight_decay = float(study.best_params['weight_decay'])\n","best_epoch = int(study.best_params['num_train_epochs'])\n","print_custom('Extract best study params')\n","\n","print(f'The best learning rate is: {best_lr}')\n","print(f'The best weight decay is: {best_weight_decay}')\n","print(f'The best epoch is : {best_epoch}')\n","\n","print_custom('Create dictionary of the best hyperparameters')\n","best_hp_dict = {\n","    'best_learning_rate' : best_lr,\n","    'best_weight_decay': best_weight_decay,\n","    'best_epoch': best_epoch\n","}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"LPbEZKgbS_WR","executionInfo":{"status":"ok","timestamp":1684021574371,"user_tz":-120,"elapsed":1274080,"user":{"displayName":"Upasana Chakraborty","userId":"16670799179179629238"}},"outputId":"224e1f9c-e55c-482c-8da8-6a6f0ca39201"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['OBJECTIFICATION', 'SEXUAL-VIOLENCE']\n","train1 (6050, 2)\n","<bound method NDFrame.head of                                                   tweet          hard_label\n","0     @USER Ignora al otro, es un capullo.El problem...  [0, 0, 0, 1, 1, 0]\n","1     @USER Si comicsgate se parece en algo a gamerg...  [0, 0, 1, 0, 0, 0]\n","2     @USER Lee sobre Gamergate, y como eso ha cambi...  [0, 0, 1, 0, 0, 0]\n","3     @USER @USER @USER Entonces como así es el merc...  [1, 0, 0, 1, 0, 1]\n","4     @USER Aaah sí. Andrew Dobson. El que se dedicó...  [0, 0, 1, 0, 0, 0]\n","...                                                 ...                 ...\n","6045  idk why y’all bitches think having half your a...  [0, 1, 0, 1, 1, 1]\n","6046  This has been a part of an experiment with @US...  [0, 0, 0, 1, 0, 0]\n","6047  \"Take me already\" \"Not yet. You gotta be ready...  [0, 0, 0, 0, 1, 0]\n","6048    @USER why do you look like a whore? /lh HTTPURL  [0, 1, 0, 1, 1, 1]\n","6049  ik when mandy says “you look like a whore” i l...  [0, 0, 0, 1, 0, 1]\n","\n","[6050 rows x 2 columns]>\n","test1 (931, 2)\n","<bound method NDFrame.head of                                                  tweet          hard_label\n","0    @USER @USER No me acuerdo de los detalles de G...  [1, 1, 0, 0, 0, 1]\n","1    @USER lo digo cada pocos dias y lo repito: TOD...  [0, 0, 1, 0, 0, 0]\n","2    Also mientras les decia eso la señalaba y deci...  [0, 0, 0, 0, 1, 0]\n","3    And all people killed,  attacked, harassed by ...  [0, 0, 1, 0, 0, 0]\n","4    On this #WorldPressFreedomDay I’m thinking of ...  [0, 0, 1, 0, 0, 0]\n","..                                                 ...                 ...\n","926  @USER “Don’t wear a black bra with a white ves...  [0, 1, 0, 1, 1, 1]\n","927  \" get changed , you look like a prostitute . \"...  [1, 0, 0, 1, 1, 0]\n","928  made this top and my mom gave me the “you look...  [0, 0, 0, 1, 0, 0]\n","929  @USER I haven't seen anything that makes you l...  [1, 1, 0, 1, 0, 1]\n","930  @USER You look like a whore in ur new picture....  [1, 1, 0, 1, 1, 0]\n","\n","[931 rows x 2 columns]>\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at sdadas/xlm-roberta-large-twitter were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at sdadas/xlm-roberta-large-twitter and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[32m[I 2023-05-13 23:25:24,205]\u001b[0m A new study created in memory with name: hp-search-task1-hard\u001b[0m\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='1396' max='1396' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1396/1396 20:47, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.379900</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.285000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2023-05-13 23:46:13,792]\u001b[0m Trial 0 finished with value: 0.308423602478552 and parameters: {'learning_rate': 1.5592115324779907e-05, 'weight_decay': 0.00019014604610473898, 'num_train_epochs': 2}. Best is trial 0 with value: 0.308423602478552.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Finding study best parameters\n","----------------------------------------------------------------------------------------------------\n","\n","\n","Extract best study params\n","----------------------------------------------------------------------------------------------------\n","The best learning rate is: 1.5592115324779907e-05\n","The best weight decay is: 0.00019014604610473898\n","The best epoch is : 2\n","\n","\n","Create dictionary of the best hyperparameters\n","----------------------------------------------------------------------------------------------------\n"]}]},{"cell_type":"markdown","source":["## construct optuna all tasks soft"],"metadata":{"id":"ZLIGIp0OFSwS"}},{"cell_type":"markdown","source":["### task1 soft"],"metadata":{"id":"D1-j-RTIFc11"}},{"cell_type":"code","source":["#relevant functions\n","import re\n","# Create function for printing \n","def print_custom(text):\n","    print('\\n')\n","    print(text)\n","    print('-'*100)\n","    \n","def simple_preprocess(text):\n","  \"\"\"\n","  pass the tweet data as a series. do not use apply function\n","  only preprocesses for replacing @USER and URLS\n","  \"\"\"\n","  # print(\"i am preprocessing\")\n","  URL_RE = re.compile(r\"https?:\\/\\/[\\w\\.\\/\\?\\=\\d&#%_:/-]+\")\n","  HANDLE_RE = re.compile(r\"@\\w+\")\n","  tweets = []\n","  for t in text:\n","    t = HANDLE_RE.sub(\"@USER\", t)\n","    t = URL_RE.sub(\"HTTPURL\", t)\n","    tweets.append(t)\n","  return tweets\n","\n","def tweet_preprocessor_spanish(text):\n","    \"\"\"\n","    imported from pysentimiento \n","    does the following: \n","    1. changes to @usario\n","    2. shortens characters repetitions to 3 (can be changed to two or more)\n","    3. converts emojis to text type of need\n","    4. normalises laughter expressions\n","    5. handles hashtags -- removes the #\n","    \"\"\"\n","    from pysentimiento.preprocessing import preprocess_tweet\n","    return preprocess_tweet(text, lang=\"es\", shorten=3)\n","\n","def tweet_preprocessor_english(text):\n","    \"\"\"\n","    imported from pysentimiento \n","    does the following: \n","    1. changes to @usario\n","    2. shortens characters repetitions to 3 (can be changed to two or more)\n","    3. converts emojis to text type of need\n","    4. normalises laughter expressions\n","    5. handles hashtags -- removes the #\n","    \"\"\"\n","    from pysentimiento.preprocessing import preprocess_tweet\n","    return preprocess_tweet(text, lang=\"en\", shorten=3)\n","\n","from sklearn.preprocessing import MultiLabelBinarizer\n","\n","# Instantiate multi-label encoder\n","  \n","def convert_logits_to_list(logits_dict):\n","    logits_dict = eval(logits_dict)\n","    logits_list = [logits_dict[\"YES\"], logits_dict[\"NO\"]]\n","    return logits_list\n","\n","def convert_list_to_logits(logits_list):\n","    logits_dict = {\"YES\": logits_list[0], \"NO\": logits_list[1]}\n","    return logits_dict\n","\n","def check_dtype(given_data):\n","    return eval(given_data)\n","\n","import optuna \n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer  \n","import os.path\n","from os import path\n","\n","from transformers import XLMRobertaForSequenceClassification, XLMRobertaTokenizer, TrainingArguments, Trainer\n","from datasets import Dataset\n","\n","#load given data\n","#load given data\n","import pandas as pd\n","og_train1 = pd.read_csv(\"train_data_task1.csv\")\n","og_dev1 = pd.read_csv(\"dev_data_task1.csv\")\n","\n","og_train1[\"soft_label\"] = og_train1[\"soft_label\"].apply(convert_logits_to_list)\n","og_train1[\"tweet\"] = simple_preprocess(og_train1[\"tweet\"])\n","# og_train1[\"soft_label\"] = og_train1[\"soft_label\"].apply(check_dtype) \n","train1_df = og_train1[[\"tweet\",\"soft_label\"]].dropna()\n","\n","\n","og_dev1[\"soft_label\"] = og_dev1[\"soft_label\"].apply(convert_logits_to_list)\n","og_dev1[\"tweet\"] = simple_preprocess(og_dev1[\"tweet\"])\n","# og_dev1[\"soft_label\"] = og_dev1[\"soft_label\"].apply(check_dtype) \n","test1_df = og_dev1[[\"tweet\",\"soft_label\"]].dropna()\n","\n","\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","# Combine train1df and test1df into a single dataframe\n","combined_df = pd.concat([train1_df, test1_df], ignore_index=True)\n","\n","# Shuffle the combined dataframe\n","combined_df_shuffled = combined_df.sample(frac=1, random_state=42)\n","\n","# Split the shuffled dataframe into train, validation, and test dataframes with an 80-10-10 split\n","train_df, temp_df = train_test_split(combined_df_shuffled, test_size=0.2, random_state=42)\n","val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n","\n","# Reset the indices of the dataframes\n","train_df = train_df.reset_index(drop=True)\n","val_df = val_df.reset_index(drop=True)\n","test_df = test_df.reset_index(drop=True)\n","\n","\n","# Load the tokenizer and model\n","model_name = \"sdadas/xlm-roberta-large-twitter\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2,ignore_mismatched_sizes=True)\n","\n","# Set parameters\n","LR_MIN = 1e-6\n","LR_CEIL = 0.001\n","WD_MIN = 4e-5\n","WD_CEIL = 0.01\n","MIN_EPOCHS = 2\n","MAX_EPOCHS = 5\n","PER_DEVICE_EVAL_BATCH = 16\n","PER_DEVICE_TRAIN_BATCH = 8\n","NUM_TRIALS = 1\n","SAVE_DIR = '/content/drive/MyDrive/ml4nlp2/final_folder/optuna_models/'\n","NAME_OF_MODEL = 'sexist_tweet_predictor_task1_hard'\n","MAX_LENGTH = 128\n","\n","import torch.nn as nn\n","def convert_to_dataset(df):\n","    df = {\"text\": df['tweet'].tolist(), \"label\": df[\"soft_label\"].tolist()}\n","    dataset = Dataset.from_dict(df)\n","    return dataset\n","\n","\n","# Convert dataframe to dataset\n","train_dataset = convert_to_dataset(train_df)\n","val_dataset = convert_to_dataset(val_df)\n","test_dataset = convert_to_dataset(test_df)\n","\n","\n","# Create the datasets\n","train_encodings = tokenizer(train_dataset[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\n","train_dataset = Dataset.from_dict({\"input_ids\": train_encodings[\"input_ids\"], \"attention_mask\": train_encodings[\"attention_mask\"], \"label\": train_dataset[\"label\"]})\n","\n","val_encodings = tokenizer(val_dataset[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\n","val_dataset = Dataset.from_dict({\"input_ids\": val_encodings[\"input_ids\"], \"attention_mask\": val_encodings[\"attention_mask\"], \"label\": val_dataset[\"label\"]})\n","\n","\n","test_encodings = tokenizer(test_dataset[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\n","test_dataset = Dataset.from_dict({\"input_ids\": test_encodings[\"input_ids\"], \"attention_mask\": test_encodings[\"attention_mask\"], \"label\": test_dataset[\"label\"]})\n","\n","def custom_loss_fn(logits, soft_labels):\n","    probs = F.softmax(logits, dim=1)\n","    # Apply nn.CrossEntropyLoss\n","    loss = nn.CrossEntropyLoss(reduction=\"sum\",label_smoothing=0.15)(probs, soft_labels)\n","    return loss\n","\n","\n","class CustomTrainer(Trainer):\n","    def compute_loss(self, model, inputs, return_outputs=False):\n","        labels = inputs.pop(\"labels\")\n","        outputs = model(**inputs)\n","        logits = outputs.logits\n","        loss = custom_loss_fn(logits, labels)\n","        return (loss, outputs) if return_outputs else loss\n","\n","\n","def objective(trial):\n","    training_args = TrainingArguments(\n","        output_dir=SAVE_DIR,\n","        learning_rate=trial.suggest_loguniform('learning_rate', LR_MIN, LR_CEIL),\n","        weight_decay=trial.suggest_loguniform('weight_decay', WD_MIN, WD_CEIL),\n","        num_train_epochs=trial.suggest_int('num_train_epochs', MIN_EPOCHS, MAX_EPOCHS),\n","        per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH,\n","        per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH,\n","        disable_tqdm=False\n","    )\n","\n","    trainer = CustomTrainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_dataset,\n","        eval_dataset=val_dataset\n","    )\n","\n","    result = trainer.train()\n","    return result.training_loss\n","\n","study = optuna.create_study(study_name='hp-search-task1-hard', direction='minimize')\n","study.optimize(objective, n_trials=NUM_TRIALS)\n","\n","print_custom('Finding study best parameters')\n","best_lr = float(study.best_params['learning_rate'])\n","best_weight_decay = float(study.best_params['weight_decay'])\n","best_epoch = int(study.best_params['num_train_epochs'])\n","print_custom('Extract best study params')\n","\n","print(f'The best learning rate is: {best_lr}')\n","print(f'The best weight decay is: {best_weight_decay}')\n","print(f'The best epoch is : {best_epoch}')\n","\n","print_custom('Create dictionary of the best hyperparameters')\n","best_hp_dict = {\n","    'best_learning_rate' : best_lr,\n","    'best_weight_decay': best_weight_decay,\n","    'best_epoch': best_epoch\n","}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":603},"id":"uxbwseCZFcIg","executionInfo":{"status":"ok","timestamp":1684055529030,"user_tz":-120,"elapsed":2243804,"user":{"displayName":"Upasana Chakraborty","userId":"16670799179179629238"}},"outputId":"dac53e57-3d00-4958-838f-8223b1b0c8f1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at sdadas/xlm-roberta-large-twitter were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at sdadas/xlm-roberta-large-twitter and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[32m[I 2023-05-14 08:34:53,061]\u001b[0m A new study created in memory with name: hp-search-task1-hard\u001b[0m\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='2388' max='2388' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2388/2388 37:09, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>4.967700</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>4.688100</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>4.635300</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>4.488400</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2023-05-14 09:12:07,328]\u001b[0m Trial 0 finished with value: 4.657868658477937 and parameters: {'learning_rate': 2.5174100002380197e-05, 'weight_decay': 0.009582405316762398, 'num_train_epochs': 3}. Best is trial 0 with value: 4.657868658477937.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Finding study best parameters\n","----------------------------------------------------------------------------------------------------\n","\n","\n","Extract best study params\n","----------------------------------------------------------------------------------------------------\n","The best learning rate is: 2.5174100002380197e-05\n","The best weight decay is: 0.009582405316762398\n","The best epoch is : 3\n","\n","\n","Create dictionary of the best hyperparameters\n","----------------------------------------------------------------------------------------------------\n"]}]},{"cell_type":"markdown","source":["### task2 soft"],"metadata":{"id":"_kA3KuqZYjWb"}},{"cell_type":"code","source":["#relevant functions\n","import re\n","# Create function for printing \n","def print_custom(text):\n","    print('\\n')\n","    print(text)\n","    print('-'*100)\n","    \n","def simple_preprocess(text):\n","  \"\"\"\n","  pass the tweet data as a series. do not use apply function\n","  only preprocesses for replacing @USER and URLS\n","  \"\"\"\n","  # print(\"i am preprocessing\")\n","  URL_RE = re.compile(r\"https?:\\/\\/[\\w\\.\\/\\?\\=\\d&#%_:/-]+\")\n","  HANDLE_RE = re.compile(r\"@\\w+\")\n","  tweets = []\n","  for t in text:\n","    t = HANDLE_RE.sub(\"@USER\", t)\n","    t = URL_RE.sub(\"HTTPURL\", t)\n","    tweets.append(t)\n","  return tweets\n","\n","def tweet_preprocessor_spanish(text):\n","    \"\"\"\n","    imported from pysentimiento \n","    does the following: \n","    1. changes to @usario\n","    2. shortens characters repetitions to 3 (can be changed to two or more)\n","    3. converts emojis to text type of need\n","    4. normalises laughter expressions\n","    5. handles hashtags -- removes the #\n","    \"\"\"\n","    from pysentimiento.preprocessing import preprocess_tweet\n","    return preprocess_tweet(text, lang=\"es\", shorten=3)\n","\n","def tweet_preprocessor_english(text):\n","    \"\"\"\n","    imported from pysentimiento \n","    does the following: \n","    1. changes to @usario\n","    2. shortens characters repetitions to 3 (can be changed to two or more)\n","    3. converts emojis to text type of need\n","    4. normalises laughter expressions\n","    5. handles hashtags -- removes the #\n","    \"\"\"\n","    from pysentimiento.preprocessing import preprocess_tweet\n","    return preprocess_tweet(text, lang=\"en\", shorten=3)\n","\n","from sklearn.preprocessing import MultiLabelBinarizer\n","\n","def convert_logits_to_list(logits_dict):\n","    \"\"\"\n","    order:\n","    1. classes are \n","    REPORTED, NO, DIRECT, JUDGEMENTAL \n","    \"\"\"\n","    logits_dict = eval(logits_dict)\n","    logits_list = [logits_dict[\"REPORTED\"], logits_dict[\"NO\"],logits_dict[\"DIRECT\"],logits_dict[\"JUDGEMENTAL\"]]\n","    return logits_list\n","\n","def convert_list_to_logits(logits_list):\n","    logits_dict = {\"REPORTED\": logits_list[0], \"NO\": logits_list[1],\"DIRECT\": logits_list[2], \"JUDGEMENTAL\": logits_list[3]}\n","    return logits_dict\n","\n","def check_dtype(given_data):\n","    return eval(given_data)\n","\n","import optuna \n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer  \n","import os.path\n","from os import path\n","\n","from transformers import XLMRobertaForSequenceClassification, XLMRobertaTokenizer, TrainingArguments, Trainer\n","from datasets import Dataset\n","\n","#load given data\n","import pandas as pd\n","og_train1 = pd.read_csv(\"train_data_task2.csv\")\n","og_dev1 = pd.read_csv(\"dev_data_task2.csv\")\n","\n","og_train1[\"soft_label\"] = og_train1[\"soft_label\"].apply(convert_logits_to_list)\n","og_train1[\"tweet\"] = simple_preprocess(og_train1[\"tweet\"])\n","# og_train1[\"soft_label\"] = og_train1[\"soft_label\"].apply(check_dtype) \n","train1_df = og_train1[[\"tweet\",\"soft_label\"]].dropna()\n","\n","\n","og_dev1[\"soft_label\"] = og_dev1[\"soft_label\"].apply(convert_logits_to_list)\n","og_dev1[\"tweet\"] = simple_preprocess(og_dev1[\"tweet\"])\n","# og_dev1[\"soft_label\"] = og_dev1[\"soft_label\"].apply(check_dtype) \n","test1_df = og_dev1[[\"tweet\",\"soft_label\"]].dropna()\n","\n","# print(\"train1\",train1_df.shape)\n","print(train1_df.head)\n","# print(\"test1\",test1_df.shape)\n","print(test1_df.head)\n","\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","# Combine train1df and test1df into a single dataframe\n","combined_df = pd.concat([train1_df, test1_df], ignore_index=True)\n","\n","# Shuffle the combined dataframe\n","combined_df_shuffled = combined_df.sample(frac=1, random_state=42)\n","\n","# Split the shuffled dataframe into train, validation, and test dataframes with an 80-10-10 split\n","train_df, temp_df = train_test_split(combined_df_shuffled, test_size=0.2, random_state=42)\n","val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n","\n","# Reset the indices of the dataframes\n","train_df = train_df.reset_index(drop=True)\n","val_df = val_df.reset_index(drop=True)\n","test_df = test_df.reset_index(drop=True)\n","\n","\n","# Load the tokenizer and model\n","model_name = \"sdadas/xlm-roberta-large-twitter\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=4,ignore_mismatched_sizes=True)\n","\n","# Set parameters\n","LR_MIN = 1e-6\n","LR_CEIL = 0.001\n","WD_MIN = 4e-5\n","WD_CEIL = 0.01\n","MIN_EPOCHS = 2\n","MAX_EPOCHS = 5\n","PER_DEVICE_EVAL_BATCH = 16\n","PER_DEVICE_TRAIN_BATCH = 8\n","NUM_TRIALS = 1\n","SAVE_DIR = '/content/drive/MyDrive/ml4nlp2/final_folder/optuna_models/'\n","NAME_OF_MODEL = 'sexist_tweet_predictor_task1_hard'\n","MAX_LENGTH = 128\n","\n","import torch.nn as nn\n","def convert_to_dataset(df):\n","    df = {\"text\": df['tweet'].tolist(), \"label\": df[\"soft_label\"].tolist()}\n","    dataset = Dataset.from_dict(df)\n","    return dataset\n","\n","\n","# Convert dataframe to dataset\n","train_dataset = convert_to_dataset(train_df)\n","val_dataset = convert_to_dataset(val_df)\n","test_dataset = convert_to_dataset(test_df)\n","\n","\n","# Create the datasets\n","train_encodings = tokenizer(train_dataset[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\n","train_dataset = Dataset.from_dict({\"input_ids\": train_encodings[\"input_ids\"], \"attention_mask\": train_encodings[\"attention_mask\"], \"label\": train_dataset[\"label\"]})\n","\n","val_encodings = tokenizer(val_dataset[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\n","val_dataset = Dataset.from_dict({\"input_ids\": val_encodings[\"input_ids\"], \"attention_mask\": val_encodings[\"attention_mask\"], \"label\": val_dataset[\"label\"]})\n","\n","\n","test_encodings = tokenizer(test_dataset[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\n","test_dataset = Dataset.from_dict({\"input_ids\": test_encodings[\"input_ids\"], \"attention_mask\": test_encodings[\"attention_mask\"], \"label\": test_dataset[\"label\"]})\n","\n","def custom_loss_fn(logits, soft_labels):\n","    probs = F.softmax(logits, dim=1)\n","    # Apply nn.CrossEntropyLoss\n","    loss = nn.CrossEntropyLoss(reduction=\"sum\",label_smoothing=0.15)(probs, soft_labels)\n","    return loss\n","\n","\n","class CustomTrainer(Trainer):\n","    def compute_loss(self, model, inputs, return_outputs=False):\n","        labels = inputs.pop(\"labels\")\n","        outputs = model(**inputs)\n","        logits = outputs.logits\n","        loss = custom_loss_fn(logits, labels)\n","        return (loss, outputs) if return_outputs else loss\n","\n","\n","def objective(trial):\n","    training_args = TrainingArguments(\n","        output_dir=SAVE_DIR,\n","        learning_rate=trial.suggest_loguniform('learning_rate', LR_MIN, LR_CEIL),\n","        weight_decay=trial.suggest_loguniform('weight_decay', WD_MIN, WD_CEIL),\n","        num_train_epochs=trial.suggest_int('num_train_epochs', MIN_EPOCHS, MAX_EPOCHS),\n","        per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH,\n","        per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH,\n","        disable_tqdm=False\n","    )\n","\n","    trainer = CustomTrainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_dataset,\n","        eval_dataset=val_dataset\n","    )\n","\n","    result = trainer.train()\n","    return result.training_loss\n","\n","study = optuna.create_study(study_name='hp-search-task1-hard', direction='minimize')\n","study.optimize(objective, n_trials=NUM_TRIALS)\n","\n","print_custom('Finding study best parameters')\n","best_lr = float(study.best_params['learning_rate'])\n","best_weight_decay = float(study.best_params['weight_decay'])\n","best_epoch = int(study.best_params['num_train_epochs'])\n","print_custom('Extract best study params')\n","\n","print(f'The best learning rate is: {best_lr}')\n","print(f'The best weight decay is: {best_weight_decay}')\n","print(f'The best epoch is : {best_epoch}')\n","\n","print_custom('Create dictionary of the best hyperparameters')\n","best_hp_dict = {\n","    'best_learning_rate' : best_lr,\n","    'best_weight_decay': best_weight_decay,\n","    'best_epoch': best_epoch\n","}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"LL8Cd6azYlxy","executionInfo":{"status":"ok","timestamp":1684056928570,"user_tz":-120,"elapsed":1361650,"user":{"displayName":"Upasana Chakraborty","userId":"16670799179179629238"}},"outputId":"e97b1d26-209e-4b65-cb46-134e3b254a16"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<bound method NDFrame.head of                                                   tweet  \\\n","0     @USER Ignora al otro, es un capullo.El problem...   \n","1     @USER Si comicsgate se parece en algo a gamerg...   \n","2     @USER Lee sobre Gamergate, y como eso ha cambi...   \n","3     @USER Un retraso social bastante lamentable, g...   \n","4     @USER @USER @USER Entonces como así es el merc...   \n","...                                                 ...   \n","6915  idk why y’all bitches think having half your a...   \n","6916  This has been a part of an experiment with @US...   \n","6917  \"Take me already\" \"Not yet. You gotta be ready...   \n","6918    @USER why do you look like a whore? /lh HTTPURL   \n","6919  ik when mandy says “you look like a whore” i l...   \n","\n","                                             soft_label  \n","0     [0.5, 0.16666666666666602, 0.0, 0.333333333333...  \n","1     [0.0, 0.833333333333333, 0.16666666666666602, ...  \n","2                                  [0.0, 1.0, 0.0, 0.0]  \n","3     [0.33333333333333304, 0.5, 0.16666666666666602...  \n","4     [0.16666666666666602, 0.33333333333333304, 0.1...  \n","...                                                 ...  \n","6915  [0.0, 0.0, 0.6666666666666661, 0.3333333333333...  \n","6916  [0.16666666666666602, 0.0, 0.16666666666666602...  \n","6917  [0.0, 0.33333333333333304, 0.5, 0.166666666666...  \n","6918  [0.0, 0.0, 0.833333333333333, 0.16666666666666...  \n","6919  [0.33333333333333304, 0.16666666666666602, 0.3...  \n","\n","[6920 rows x 2 columns]>\n","<bound method NDFrame.head of                                                   tweet  \\\n","0     @USER La comunidad gamer es un antro de misógi...   \n","1     @USER @USER No me acuerdo de los detalles de G...   \n","2     @USER lo digo cada pocos dias y lo repito: TOD...   \n","3     Also mientras les decia eso la señalaba y deci...   \n","4     And all people killed,  attacked, harassed by ...   \n","...                                                 ...   \n","1033  @USER “Don’t wear a black bra with a white ves...   \n","1034  \" get changed , you look like a prostitute . \"...   \n","1035  made this top and my mom gave me the “you look...   \n","1036  @USER I haven't seen anything that makes you l...   \n","1037  @USER You look like a whore in ur new picture....   \n","\n","                                             soft_label  \n","0     [0.16666666666666602, 0.5, 0.0, 0.333333333333...  \n","1     [0.33333333333333304, 0.16666666666666602, 0.0...  \n","2                                  [0.0, 1.0, 0.0, 0.0]  \n","3     [0.6666666666666661, 0.16666666666666602, 0.0,...  \n","4     [0.0, 0.833333333333333, 0.16666666666666602, ...  \n","...                                                 ...  \n","1033  [0.0, 0.0, 0.833333333333333, 0.16666666666666...  \n","1034  [0.5, 0.16666666666666602, 0.33333333333333304...  \n","1035  [0.6666666666666661, 0.16666666666666602, 0.0,...  \n","1036                               [0.0, 0.0, 1.0, 0.0]  \n","1037  [0.0, 0.0, 0.833333333333333, 0.16666666666666...  \n","\n","[1038 rows x 2 columns]>\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at sdadas/xlm-roberta-large-twitter were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at sdadas/xlm-roberta-large-twitter and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[32m[I 2023-05-14 09:13:05,152]\u001b[0m A new study created in memory with name: hp-search-task1-hard\u001b[0m\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='1592' max='1592' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1592/1592 22:20, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>9.987300</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>9.866800</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>9.969400</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2023-05-14 09:35:26,908]\u001b[0m Trial 0 finished with value: 9.94545673006144 and parameters: {'learning_rate': 0.0006429848369437259, 'weight_decay': 6.208951123230285e-05, 'num_train_epochs': 2}. Best is trial 0 with value: 9.94545673006144.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Finding study best parameters\n","----------------------------------------------------------------------------------------------------\n","\n","\n","Extract best study params\n","----------------------------------------------------------------------------------------------------\n","The best learning rate is: 0.0006429848369437259\n","The best weight decay is: 6.208951123230285e-05\n","The best epoch is : 2\n","\n","\n","Create dictionary of the best hyperparameters\n","----------------------------------------------------------------------------------------------------\n"]}]},{"cell_type":"markdown","source":["### task 3 soft"],"metadata":{"id":"SxRAP-gOYn2o"}},{"cell_type":"code","source":["#relevant functions\n","import re\n","# Create function for printing \n","def print_custom(text):\n","    print('\\n')\n","    print(text)\n","    print('-'*100)\n","    \n","def simple_preprocess(text):\n","  \"\"\"\n","  pass the tweet data as a series. do not use apply function\n","  only preprocesses for replacing @USER and URLS\n","  \"\"\"\n","  # print(\"i am preprocessing\")\n","  URL_RE = re.compile(r\"https?:\\/\\/[\\w\\.\\/\\?\\=\\d&#%_:/-]+\")\n","  HANDLE_RE = re.compile(r\"@\\w+\")\n","  tweets = []\n","  for t in text:\n","    t = HANDLE_RE.sub(\"@USER\", t)\n","    t = URL_RE.sub(\"HTTPURL\", t)\n","    tweets.append(t)\n","  return tweets\n","\n","def tweet_preprocessor_spanish(text):\n","    \"\"\"\n","    imported from pysentimiento \n","    does the following: \n","    1. changes to @usario\n","    2. shortens characters repetitions to 3 (can be changed to two or more)\n","    3. converts emojis to text type of need\n","    4. normalises laughter expressions\n","    5. handles hashtags -- removes the #\n","    \"\"\"\n","    from pysentimiento.preprocessing import preprocess_tweet\n","    return preprocess_tweet(text, lang=\"es\", shorten=3)\n","\n","def tweet_preprocessor_english(text):\n","    \"\"\"\n","    imported from pysentimiento \n","    does the following: \n","    1. changes to @usario\n","    2. shortens characters repetitions to 3 (can be changed to two or more)\n","    3. converts emojis to text type of need\n","    4. normalises laughter expressions\n","    5. handles hashtags -- removes the #\n","    \"\"\"\n","    from pysentimiento.preprocessing import preprocess_tweet\n","    return preprocess_tweet(text, lang=\"en\", shorten=3)\n","\n","from sklearn.preprocessing import MultiLabelBinarizer\n","\n","def convert_logits_to_list(logits_dict):\n","    \"\"\"\n","    order:\n","    1. classes are \n","    OBJECTIFICATION,'SEXUAL-VIOLENCE',NO,STEREOTYPING-DOMINANCE,IDEOLOGICAL-INEQUALITY,MISOGYNY-NON-SEXUAL-VIOLENCE\n","    \"\"\"\n","    logits_dict = eval(logits_dict)\n","    logits_list = [logits_dict[\"OBJECTIFICATION\"], logits_dict[\"SEXUAL-VIOLENCE\"],logits_dict[\"NO\"],logits_dict[\"STEREOTYPING-DOMINANCE\"],logits_dict[\"IDEOLOGICAL-INEQUALITY\"], logits_dict[\"MISOGYNY-NON-SEXUAL-VIOLENCE\"]]\n","    return logits_list\n","\n","def convert_list_to_logits(logits_list):\n","    logits_dict = {\"OBJECTIFICATION\": logits_list[0], \"SEXUAL-VIOLENCE\": logits_list[1],\"NO\": logits_list[2], \"STEREOTYPING-DOMINANCE\": logits_list[3],\"IDEOLOGICAL-INEQUALITY\": logits_list[4], \"MISOGYNY-NON-SEXUAL-VIOLENCE\": logits_list[5]}\n","    return logits_dict\n","\n","def check_dtype(given_data):\n","    return eval(given_data)\n","\n","\n","import optuna \n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer  \n","import os.path\n","from os import path\n","\n","from transformers import XLMRobertaForSequenceClassification, XLMRobertaTokenizer, TrainingArguments, Trainer\n","from datasets import Dataset\n","\n","#load given data\n","import pandas as pd\n","og_train1 = pd.read_csv(\"train_data_task3.csv\")\n","og_dev1 = pd.read_csv(\"dev_data_task3.csv\")\n","\n","og_train1[\"soft_label\"] = og_train1[\"soft_label\"].apply(convert_logits_to_list)\n","og_train1[\"tweet\"] = simple_preprocess(og_train1[\"tweet\"])\n","# og_train1[\"soft_label\"] = og_train1[\"soft_label\"].apply(check_dtype) \n","train1_df = og_train1[[\"tweet\",\"soft_label\"]].dropna()\n","\n","\n","og_dev1[\"soft_label\"] = og_dev1[\"soft_label\"].apply(convert_logits_to_list)\n","og_dev1[\"tweet\"] = simple_preprocess(og_dev1[\"tweet\"])\n","# og_dev1[\"soft_label\"] = og_dev1[\"soft_label\"].apply(check_dtype) \n","test1_df = og_dev1[[\"tweet\",\"soft_label\"]].dropna()\n","\n","# print(\"train1\",train1_df.shape)\n","print(train1_df.head)\n","# print(\"test1\",test1_df.shape)\n","print(test1_df.head)\n","\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","# Combine train1df and test1df into a single dataframe\n","combined_df = pd.concat([train1_df, test1_df], ignore_index=True)\n","\n","# Shuffle the combined dataframe\n","combined_df_shuffled = combined_df.sample(frac=1, random_state=42)\n","\n","# Split the shuffled dataframe into train, validation, and test dataframes with an 80-10-10 split\n","train_df, temp_df = train_test_split(combined_df_shuffled, test_size=0.2, random_state=42)\n","val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n","\n","# Reset the indices of the dataframes\n","train_df = train_df.reset_index(drop=True)\n","val_df = val_df.reset_index(drop=True)\n","test_df = test_df.reset_index(drop=True)\n","\n","\n","# Load the tokenizer and model\n","model_name = \"sdadas/xlm-roberta-large-twitter\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=6,ignore_mismatched_sizes=True)\n","\n","# Set parameters\n","LR_MIN = 1e-6\n","LR_CEIL = 0.001\n","WD_MIN = 4e-5\n","WD_CEIL = 0.01\n","MIN_EPOCHS = 2\n","MAX_EPOCHS = 5\n","PER_DEVICE_EVAL_BATCH = 16\n","PER_DEVICE_TRAIN_BATCH = 8\n","NUM_TRIALS = 1\n","SAVE_DIR = '/content/drive/MyDrive/ml4nlp2/final_folder/optuna_models/'\n","NAME_OF_MODEL = 'sexist_tweet_predictor_task1_hard'\n","MAX_LENGTH = 128\n","\n","import torch.nn as nn\n","def convert_to_dataset(df):\n","    df = {\"text\": df['tweet'].tolist(), \"label\": df[\"soft_label\"].tolist()}\n","    dataset = Dataset.from_dict(df)\n","    return dataset\n","\n","\n","# Convert dataframe to dataset\n","train_dataset = convert_to_dataset(train_df)\n","val_dataset = convert_to_dataset(val_df)\n","test_dataset = convert_to_dataset(test_df)\n","\n","\n","# Create the datasets\n","train_encodings = tokenizer(train_dataset[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\n","train_dataset = Dataset.from_dict({\"input_ids\": train_encodings[\"input_ids\"], \"attention_mask\": train_encodings[\"attention_mask\"], \"label\": train_dataset[\"label\"]})\n","\n","val_encodings = tokenizer(val_dataset[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\n","val_dataset = Dataset.from_dict({\"input_ids\": val_encodings[\"input_ids\"], \"attention_mask\": val_encodings[\"attention_mask\"], \"label\": val_dataset[\"label\"]})\n","\n","\n","test_encodings = tokenizer(test_dataset[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\n","test_dataset = Dataset.from_dict({\"input_ids\": test_encodings[\"input_ids\"], \"attention_mask\": test_encodings[\"attention_mask\"], \"label\": test_dataset[\"label\"]})\n","\n","def custom_loss_fn(logits, soft_labels):\n","    # Convert probabilities to logits using the inverse sigmoid (logit) function\n","    # eps = 1e-7  # Small constant to avoid numerical instability\n","    # logits = torch.log(probabilities.clamp(min=eps, max=1-eps) / (1 - probabilities.clamp(min=eps, max=1-eps)))\n","    loss = nn.BCEWithLogitsLoss(reduction='sum')(logits, soft_labels)\n","    return loss\n","\n","\n","class CustomTrainer(Trainer):\n","    def compute_loss(self, model, inputs, return_outputs=False):\n","        labels = inputs.pop(\"labels\")\n","        outputs = model(**inputs)\n","        logits = outputs.logits\n","        loss = custom_loss_fn(logits, labels)\n","        return (loss, outputs) if return_outputs else loss\n","\n","\n","def objective(trial):\n","    training_args = TrainingArguments(\n","        output_dir=SAVE_DIR,\n","        learning_rate=trial.suggest_loguniform('learning_rate', LR_MIN, LR_CEIL),\n","        weight_decay=trial.suggest_loguniform('weight_decay', WD_MIN, WD_CEIL),\n","        num_train_epochs=trial.suggest_int('num_train_epochs', MIN_EPOCHS, MAX_EPOCHS),\n","        per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH,\n","        per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH,\n","        disable_tqdm=False\n","    )\n","\n","    trainer = CustomTrainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_dataset,\n","        eval_dataset=val_dataset\n","    )\n","\n","    result = trainer.train()\n","    return result.training_loss\n","\n","study = optuna.create_study(study_name='hp-search-task1-hard', direction='minimize')\n","study.optimize(objective, n_trials=NUM_TRIALS)\n","\n","print_custom('Finding study best parameters')\n","best_lr = float(study.best_params['learning_rate'])\n","best_weight_decay = float(study.best_params['weight_decay'])\n","best_epoch = int(study.best_params['num_train_epochs'])\n","print_custom('Extract best study params')\n","\n","print(f'The best learning rate is: {best_lr}')\n","print(f'The best weight decay is: {best_weight_decay}')\n","print(f'The best epoch is : {best_epoch}')\n","\n","print_custom('Create dictionary of the best hyperparameters')\n","best_hp_dict = {\n","    'best_learning_rate' : best_lr,\n","    'best_weight_decay': best_weight_decay,\n","    'best_epoch': best_epoch\n","}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"pJ7D7owbYqQ3","executionInfo":{"status":"ok","timestamp":1684059069290,"user_tz":-120,"elapsed":1361895,"user":{"displayName":"Upasana Chakraborty","userId":"16670799179179629238"}},"outputId":"0fdee029-ccdb-461d-b3fb-055434310cc0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<bound method NDFrame.head of                                                   tweet  \\\n","0     @USER Ignora al otro, es un capullo.El problem...   \n","1     @USER Si comicsgate se parece en algo a gamerg...   \n","2     @USER Lee sobre Gamergate, y como eso ha cambi...   \n","3     @USER Un retraso social bastante lamentable, g...   \n","4     @USER @USER @USER Entonces como así es el merc...   \n","...                                                 ...   \n","6915  idk why y’all bitches think having half your a...   \n","6916  This has been a part of an experiment with @US...   \n","6917  \"Take me already\" \"Not yet. You gotta be ready...   \n","6918    @USER why do you look like a whore? /lh HTTPURL   \n","6919  ik when mandy says “you look like a whore” i l...   \n","\n","                                             soft_label  \n","0     [0.33333333333333304, 0.33333333333333304, 0.1...  \n","1     [0.16666666666666602, 0.0, 0.833333333333333, ...  \n","2                        [0.0, 0.0, 1.0, 0.0, 0.0, 0.0]  \n","3     [0.16666666666666602, 0.16666666666666602, 0.5...  \n","4     [0.5, 0.0, 0.33333333333333304, 0.5, 0.3333333...  \n","...                                                 ...  \n","6915  [0.6666666666666661, 0.6666666666666661, 0.0, ...  \n","6916  [0.6666666666666661, 0.0, 0.0, 0.1666666666666...  \n","6917  [0.16666666666666602, 0.33333333333333304, 0.3...  \n","6918  [0.6666666666666661, 0.5, 0.0, 0.3333333333333...  \n","6919  [0.5, 0.16666666666666602, 0.16666666666666602...  \n","\n","[6920 rows x 2 columns]>\n","<bound method NDFrame.head of                                                   tweet  \\\n","0     @USER La comunidad gamer es un antro de misógi...   \n","1     @USER @USER No me acuerdo de los detalles de G...   \n","2     @USER lo digo cada pocos dias y lo repito: TOD...   \n","3     Also mientras les decia eso la señalaba y deci...   \n","4     And all people killed,  attacked, harassed by ...   \n","...                                                 ...   \n","1033  @USER “Don’t wear a black bra with a white ves...   \n","1034  \" get changed , you look like a prostitute . \"...   \n","1035  made this top and my mom gave me the “you look...   \n","1036  @USER I haven't seen anything that makes you l...   \n","1037  @USER You look like a whore in ur new picture....   \n","\n","                                             soft_label  \n","0        [0.0, 0.16666666666666602, 0.5, 0.0, 0.0, 0.5]  \n","1     [0.16666666666666602, 0.0, 0.16666666666666602...  \n","2                        [0.0, 0.0, 1.0, 0.0, 0.0, 0.0]  \n","3     [0.0, 0.6666666666666661, 0.16666666666666602,...  \n","4     [0.0, 0.0, 0.833333333333333, 0.16666666666666...  \n","...                                                 ...  \n","1033  [0.833333333333333, 0.33333333333333304, 0.0, ...  \n","1034  [0.6666666666666661, 0.833333333333333, 0.1666...  \n","1035  [0.833333333333333, 0.0, 0.16666666666666602, ...  \n","1036  [0.6666666666666661, 0.16666666666666602, 0.0,...  \n","1037  [0.5, 0.33333333333333304, 0.0, 0.166666666666...  \n","\n","[1038 rows x 2 columns]>\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at sdadas/xlm-roberta-large-twitter were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at sdadas/xlm-roberta-large-twitter and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[32m[I 2023-05-14 09:48:44,729]\u001b[0m A new study created in memory with name: hp-search-task1-hard\u001b[0m\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='1592' max='1592' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1592/1592 22:21, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>21.516400</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>18.906800</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>18.440300</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2023-05-14 10:11:07,560]\u001b[0m Trial 0 finished with value: 19.555706311709915 and parameters: {'learning_rate': 1.121348453013019e-06, 'weight_decay': 0.0005161669504916949, 'num_train_epochs': 2}. Best is trial 0 with value: 19.555706311709915.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Finding study best parameters\n","----------------------------------------------------------------------------------------------------\n","\n","\n","Extract best study params\n","----------------------------------------------------------------------------------------------------\n","The best learning rate is: 1.121348453013019e-06\n","The best weight decay is: 0.0005161669504916949\n","The best epoch is : 2\n","\n","\n","Create dictionary of the best hyperparameters\n","----------------------------------------------------------------------------------------------------\n"]}]},{"cell_type":"code","source":["# with a different custom loss function\n","#relevant functions\n","import re\n","# Create function for printing \n","def print_custom(text):\n","    print('\\n')\n","    print(text)\n","    print('-'*100)\n","    \n","def simple_preprocess(text):\n","  \"\"\"\n","  pass the tweet data as a series. do not use apply function\n","  only preprocesses for replacing @USER and URLS\n","  \"\"\"\n","  # print(\"i am preprocessing\")\n","  URL_RE = re.compile(r\"https?:\\/\\/[\\w\\.\\/\\?\\=\\d&#%_:/-]+\")\n","  HANDLE_RE = re.compile(r\"@\\w+\")\n","  tweets = []\n","  for t in text:\n","    t = HANDLE_RE.sub(\"@USER\", t)\n","    t = URL_RE.sub(\"HTTPURL\", t)\n","    tweets.append(t)\n","  return tweets\n","\n","def tweet_preprocessor_spanish(text):\n","    \"\"\"\n","    imported from pysentimiento \n","    does the following: \n","    1. changes to @usario\n","    2. shortens characters repetitions to 3 (can be changed to two or more)\n","    3. converts emojis to text type of need\n","    4. normalises laughter expressions\n","    5. handles hashtags -- removes the #\n","    \"\"\"\n","    from pysentimiento.preprocessing import preprocess_tweet\n","    return preprocess_tweet(text, lang=\"es\", shorten=3)\n","\n","def tweet_preprocessor_english(text):\n","    \"\"\"\n","    imported from pysentimiento \n","    does the following: \n","    1. changes to @usario\n","    2. shortens characters repetitions to 3 (can be changed to two or more)\n","    3. converts emojis to text type of need\n","    4. normalises laughter expressions\n","    5. handles hashtags -- removes the #\n","    \"\"\"\n","    from pysentimiento.preprocessing import preprocess_tweet\n","    return preprocess_tweet(text, lang=\"en\", shorten=3)\n","\n","from sklearn.preprocessing import MultiLabelBinarizer\n","\n","def convert_logits_to_list(logits_dict):\n","    \"\"\"\n","    order:\n","    1. classes are \n","    OBJECTIFICATION,'SEXUAL-VIOLENCE',NO,STEREOTYPING-DOMINANCE,IDEOLOGICAL-INEQUALITY,MISOGYNY-NON-SEXUAL-VIOLENCE\n","    \"\"\"\n","    logits_dict = eval(logits_dict)\n","    logits_list = [logits_dict[\"OBJECTIFICATION\"], logits_dict[\"SEXUAL-VIOLENCE\"],logits_dict[\"NO\"],logits_dict[\"STEREOTYPING-DOMINANCE\"],logits_dict[\"IDEOLOGICAL-INEQUALITY\"], logits_dict[\"MISOGYNY-NON-SEXUAL-VIOLENCE\"]]\n","    return logits_list\n","\n","def convert_list_to_logits(logits_list):\n","    logits_dict = {\"OBJECTIFICATION\": logits_list[0], \"SEXUAL-VIOLENCE\": logits_list[1],\"NO\": logits_list[2], \"STEREOTYPING-DOMINANCE\": logits_list[3],\"IDEOLOGICAL-INEQUALITY\": logits_list[4], \"MISOGYNY-NON-SEXUAL-VIOLENCE\": logits_list[5]}\n","    return logits_dict\n","\n","def check_dtype(given_data):\n","    return eval(given_data)\n","\n","\n","import optuna \n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer  \n","import os.path\n","from os import path\n","\n","from transformers import XLMRobertaForSequenceClassification, XLMRobertaTokenizer, TrainingArguments, Trainer\n","from datasets import Dataset\n","\n","#load given data\n","import pandas as pd\n","og_train1 = pd.read_csv(\"train_data_task3.csv\")\n","og_dev1 = pd.read_csv(\"dev_data_task3.csv\")\n","\n","og_train1[\"soft_label\"] = og_train1[\"soft_label\"].apply(convert_logits_to_list)\n","og_train1[\"tweet\"] = simple_preprocess(og_train1[\"tweet\"])\n","# og_train1[\"soft_label\"] = og_train1[\"soft_label\"].apply(check_dtype) \n","train1_df = og_train1[[\"tweet\",\"soft_label\"]].dropna()\n","\n","\n","og_dev1[\"soft_label\"] = og_dev1[\"soft_label\"].apply(convert_logits_to_list)\n","og_dev1[\"tweet\"] = simple_preprocess(og_dev1[\"tweet\"])\n","# og_dev1[\"soft_label\"] = og_dev1[\"soft_label\"].apply(check_dtype) \n","test1_df = og_dev1[[\"tweet\",\"soft_label\"]].dropna()\n","\n","# print(\"train1\",train1_df.shape)\n","print(train1_df.head)\n","# print(\"test1\",test1_df.shape)\n","print(test1_df.head)\n","\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","# Combine train1df and test1df into a single dataframe\n","combined_df = pd.concat([train1_df, test1_df], ignore_index=True)\n","\n","# Shuffle the combined dataframe\n","combined_df_shuffled = combined_df.sample(frac=1, random_state=42)\n","\n","# Split the shuffled dataframe into train, validation, and test dataframes with an 80-10-10 split\n","train_df, temp_df = train_test_split(combined_df_shuffled, test_size=0.2, random_state=42)\n","val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n","\n","# Reset the indices of the dataframes\n","train_df = train_df.reset_index(drop=True)\n","val_df = val_df.reset_index(drop=True)\n","test_df = test_df.reset_index(drop=True)\n","\n","\n","# Load the tokenizer and model\n","model_name = \"sdadas/xlm-roberta-large-twitter\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=6,ignore_mismatched_sizes=True)\n","\n","# Set parameters\n","LR_MIN = 1e-6\n","LR_CEIL = 0.001\n","WD_MIN = 4e-5\n","WD_CEIL = 0.01\n","MIN_EPOCHS = 2\n","MAX_EPOCHS = 5\n","PER_DEVICE_EVAL_BATCH = 16\n","PER_DEVICE_TRAIN_BATCH = 8\n","NUM_TRIALS = 1\n","SAVE_DIR = '/content/drive/MyDrive/ml4nlp2/final_folder/optuna_models/'\n","NAME_OF_MODEL = 'sexist_tweet_predictor_task1_hard'\n","MAX_LENGTH = 128\n","\n","import torch.nn as nn\n","def convert_to_dataset(df):\n","    df = {\"text\": df['tweet'].tolist(), \"label\": df[\"soft_label\"].tolist()}\n","    dataset = Dataset.from_dict(df)\n","    return dataset\n","\n","\n","# Convert dataframe to dataset\n","train_dataset = convert_to_dataset(train_df)\n","val_dataset = convert_to_dataset(val_df)\n","test_dataset = convert_to_dataset(test_df)\n","\n","\n","# Create the datasets\n","train_encodings = tokenizer(train_dataset[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\n","train_dataset = Dataset.from_dict({\"input_ids\": train_encodings[\"input_ids\"], \"attention_mask\": train_encodings[\"attention_mask\"], \"label\": train_dataset[\"label\"]})\n","\n","val_encodings = tokenizer(val_dataset[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\n","val_dataset = Dataset.from_dict({\"input_ids\": val_encodings[\"input_ids\"], \"attention_mask\": val_encodings[\"attention_mask\"], \"label\": val_dataset[\"label\"]})\n","\n","\n","test_encodings = tokenizer(test_dataset[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\n","test_dataset = Dataset.from_dict({\"input_ids\": test_encodings[\"input_ids\"], \"attention_mask\": test_encodings[\"attention_mask\"], \"label\": test_dataset[\"label\"]})\n","\n","#with cross entropy loss and label smoothing\n","def custom_loss_fn(logits, soft_labels):\n","    probs = torch.sigmoid(logits)\n","    # Apply nn.CrossEntropyLoss\n","    loss = nn.CrossEntropyLoss(reduction=\"sum\",label_smoothing=0.15)(probs, soft_labels)\n","    return loss\n","\n","\n","class CustomTrainer(Trainer):\n","    def compute_loss(self, model, inputs, return_outputs=False):\n","        labels = inputs.pop(\"labels\")\n","        outputs = model(**inputs)\n","        logits = outputs.logits\n","        loss = custom_loss_fn(logits, labels)\n","        return (loss, outputs) if return_outputs else loss\n","\n","\n","def objective(trial):\n","    training_args = TrainingArguments(\n","        output_dir=SAVE_DIR,\n","        learning_rate=trial.suggest_loguniform('learning_rate', LR_MIN, LR_CEIL),\n","        weight_decay=trial.suggest_loguniform('weight_decay', WD_MIN, WD_CEIL),\n","        num_train_epochs=trial.suggest_int('num_train_epochs', MIN_EPOCHS, MAX_EPOCHS),\n","        per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH,\n","        per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH,\n","        disable_tqdm=False\n","    )\n","\n","    trainer = CustomTrainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_dataset,\n","        eval_dataset=val_dataset\n","    )\n","\n","    result = trainer.train()\n","    return result.training_loss\n","\n","study = optuna.create_study(study_name='hp-search-task1-hard', direction='minimize')\n","study.optimize(objective, n_trials=NUM_TRIALS)\n","\n","print_custom('Finding study best parameters')\n","best_lr = float(study.best_params['learning_rate'])\n","best_weight_decay = float(study.best_params['weight_decay'])\n","best_epoch = int(study.best_params['num_train_epochs'])\n","print_custom('Extract best study params')\n","\n","print(f'The best learning rate is: {best_lr}')\n","print(f'The best weight decay is: {best_weight_decay}')\n","print(f'The best epoch is : {best_epoch}')\n","\n","print_custom('Create dictionary of the best hyperparameters')\n","best_hp_dict = {\n","    'best_learning_rate' : best_lr,\n","    'best_weight_decay': best_weight_decay,\n","    'best_epoch': best_epoch\n","}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"wTJa4RkhvbaG","executionInfo":{"status":"ok","timestamp":1684061107958,"user_tz":-120,"elapsed":2026042,"user":{"displayName":"Upasana Chakraborty","userId":"16670799179179629238"}},"outputId":"f3c4eb97-290b-447d-eb52-f8ef773b37c9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<bound method NDFrame.head of                                                   tweet  \\\n","0     @USER Ignora al otro, es un capullo.El problem...   \n","1     @USER Si comicsgate se parece en algo a gamerg...   \n","2     @USER Lee sobre Gamergate, y como eso ha cambi...   \n","3     @USER Un retraso social bastante lamentable, g...   \n","4     @USER @USER @USER Entonces como así es el merc...   \n","...                                                 ...   \n","6915  idk why y’all bitches think having half your a...   \n","6916  This has been a part of an experiment with @US...   \n","6917  \"Take me already\" \"Not yet. You gotta be ready...   \n","6918    @USER why do you look like a whore? /lh HTTPURL   \n","6919  ik when mandy says “you look like a whore” i l...   \n","\n","                                             soft_label  \n","0     [0.33333333333333304, 0.33333333333333304, 0.1...  \n","1     [0.16666666666666602, 0.0, 0.833333333333333, ...  \n","2                        [0.0, 0.0, 1.0, 0.0, 0.0, 0.0]  \n","3     [0.16666666666666602, 0.16666666666666602, 0.5...  \n","4     [0.5, 0.0, 0.33333333333333304, 0.5, 0.3333333...  \n","...                                                 ...  \n","6915  [0.6666666666666661, 0.6666666666666661, 0.0, ...  \n","6916  [0.6666666666666661, 0.0, 0.0, 0.1666666666666...  \n","6917  [0.16666666666666602, 0.33333333333333304, 0.3...  \n","6918  [0.6666666666666661, 0.5, 0.0, 0.3333333333333...  \n","6919  [0.5, 0.16666666666666602, 0.16666666666666602...  \n","\n","[6920 rows x 2 columns]>\n","<bound method NDFrame.head of                                                   tweet  \\\n","0     @USER La comunidad gamer es un antro de misógi...   \n","1     @USER @USER No me acuerdo de los detalles de G...   \n","2     @USER lo digo cada pocos dias y lo repito: TOD...   \n","3     Also mientras les decia eso la señalaba y deci...   \n","4     And all people killed,  attacked, harassed by ...   \n","...                                                 ...   \n","1033  @USER “Don’t wear a black bra with a white ves...   \n","1034  \" get changed , you look like a prostitute . \"...   \n","1035  made this top and my mom gave me the “you look...   \n","1036  @USER I haven't seen anything that makes you l...   \n","1037  @USER You look like a whore in ur new picture....   \n","\n","                                             soft_label  \n","0        [0.0, 0.16666666666666602, 0.5, 0.0, 0.0, 0.5]  \n","1     [0.16666666666666602, 0.0, 0.16666666666666602...  \n","2                        [0.0, 0.0, 1.0, 0.0, 0.0, 0.0]  \n","3     [0.0, 0.6666666666666661, 0.16666666666666602,...  \n","4     [0.0, 0.0, 0.833333333333333, 0.16666666666666...  \n","...                                                 ...  \n","1033  [0.833333333333333, 0.33333333333333304, 0.0, ...  \n","1034  [0.6666666666666661, 0.833333333333333, 0.1666...  \n","1035  [0.833333333333333, 0.0, 0.16666666666666602, ...  \n","1036  [0.6666666666666661, 0.16666666666666602, 0.0,...  \n","1037  [0.5, 0.33333333333333304, 0.0, 0.166666666666...  \n","\n","[1038 rows x 2 columns]>\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at sdadas/xlm-roberta-large-twitter were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at sdadas/xlm-roberta-large-twitter and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[32m[I 2023-05-14 10:11:40,324]\u001b[0m A new study created in memory with name: hp-search-task1-hard\u001b[0m\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='2388' max='2388' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2388/2388 33:24, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>14.640100</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>13.983600</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>14.043500</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>13.821900</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2023-05-14 10:45:06,110]\u001b[0m Trial 0 finished with value: 14.076958482189594 and parameters: {'learning_rate': 8.036768439133452e-06, 'weight_decay': 8.398333136282262e-05, 'num_train_epochs': 3}. Best is trial 0 with value: 14.076958482189594.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Finding study best parameters\n","----------------------------------------------------------------------------------------------------\n","\n","\n","Extract best study params\n","----------------------------------------------------------------------------------------------------\n","The best learning rate is: 8.036768439133452e-06\n","The best weight decay is: 8.398333136282262e-05\n","The best epoch is : 3\n","\n","\n","Create dictionary of the best hyperparameters\n","----------------------------------------------------------------------------------------------------\n"]}]}]}