{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","os.chdir(\"/content/drive/MyDrive/ml4nlp2/final_folder\")\n","!ls\n","\n","from  IPython. display  import  clear_output\n","import warnings\n","warnings. filterwarnings('ignore')\n","\n","\n","# installs\n","!pip install datasets\n","!pip install transformers==4.28.0\n","!pip install sentencepiece==0.1.98\n","!pip install pysentimiento"],"metadata":{"id":"xahQQL45_yDM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684183947277,"user_tz":-120,"elapsed":54821,"user":{"displayName":"Rohit Koonireddy","userId":"17353448389884117833"}},"outputId":"ff97ad73-e1dd-4b46-faec-6c055a0581be"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"," baselines\t\t\t     large_train_data_task1_en.csv\n"," converted_og\t\t\t     large_train_data_task1_es.csv\n"," data_process_code\t\t    'models_zoo -rohit.xlsx'\n"," dev_data_task1.csv\t\t     models_zoo.xlsx\n"," dev_data_task1_en.csv\t\t     og_data\n"," dev_data_task1_es.csv\t\t     output_hard1.json\n"," dev_data_task2.csv\t\t     output_soft1.json\n"," dev_data_task2_en.csv\t\t     rohit_final_code\n"," dev_data_task2_es.csv\t\t     train_data_task1.csv\n"," dev_data_task3.csv\t\t     train_data_task1_en.csv\n"," dev_data_task3_en.csv\t\t     train_data_task1_es.csv\n"," dev_data_task3_es.csv\t\t     train_data_task2.csv\n"," exist2023evaluation.py\t\t     train_data_task2_en.csv\n"," EXIST2023_test_clean.json\t     train_data_task2_es.csv\n"," golds\t\t\t\t     train_data_task3.csv\n"," good_paper_distilled_learning.pdf   train_data_task3_en.csv\n"," hp_optune\t\t\t     train_data_task3_es.csv\n"," large_train_data_task1.csv\t     translated_og\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting datasets\n","  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n","Collecting dill<0.3.7,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.4.0)\n","Collecting aiohttp (from datasets)\n","  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting huggingface-hub<1.0.0,>=0.11.0 (from datasets)\n","  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n","Collecting responses<0.19 (from datasets)\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n","Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n","  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets)\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n","  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->datasets)\n","  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->datasets)\n","  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n","Installing collected packages: xxhash, multidict, frozenlist, dill, async-timeout, yarl, responses, multiprocess, huggingface-hub, aiosignal, aiohttp, datasets\n","Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.12.0 dill-0.3.6 frozenlist-1.3.3 huggingface-hub-0.14.1 multidict-6.0.4 multiprocess-0.70.14 responses-0.18.0 xxhash-3.2.0 yarl-1.9.2\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers==4.28.0\n","  Downloading transformers-4.28.0-py3-none-any.whl (7.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (3.12.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (0.14.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (2.27.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.28.0)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m109.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (2023.4.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (3.4)\n","Installing collected packages: tokenizers, transformers\n","Successfully installed tokenizers-0.13.3 transformers-4.28.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentencepiece==0.1.98\n","  Downloading sentencepiece-0.1.98-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.98\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pysentimiento\n","  Downloading pysentimiento-0.7.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: datasets>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from pysentimiento) (2.12.0)\n","Collecting emoji<2.0.0,>=1.6.1 (from pysentimiento)\n","  Downloading emoji-1.7.0.tar.gz (175 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.4/175.4 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: spacy<4.0.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from pysentimiento) (3.5.2)\n","Requirement already satisfied: torch==2.0.0 in /usr/local/lib/python3.10/dist-packages (from pysentimiento) (2.0.0+cu118)\n","Requirement already satisfied: transformers>=4.13.0 in /usr/local/lib/python3.10/dist-packages (from pysentimiento) (4.28.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->pysentimiento) (3.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->pysentimiento) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->pysentimiento) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->pysentimiento) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->pysentimiento) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->pysentimiento) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0->pysentimiento) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0->pysentimiento) (16.0.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.13.3->pysentimiento) (1.22.4)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.13.3->pysentimiento) (9.0.0)\n","Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.13.3->pysentimiento) (0.3.6)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=1.13.3->pysentimiento) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.13.3->pysentimiento) (2.27.1)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.13.3->pysentimiento) (4.65.0)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=1.13.3->pysentimiento) (3.2.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets>=1.13.3->pysentimiento) (0.70.14)\n","Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.13.3->pysentimiento) (2023.4.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=1.13.3->pysentimiento) (3.8.4)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.13.3->pysentimiento) (0.14.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets>=1.13.3->pysentimiento) (23.1)\n","Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.13.3->pysentimiento) (0.18.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.13.3->pysentimiento) (6.0)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->pysentimiento) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->pysentimiento) (1.0.4)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->pysentimiento) (1.0.9)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->pysentimiento) (2.0.7)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->pysentimiento) (3.0.8)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->pysentimiento) (8.1.9)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->pysentimiento) (1.1.1)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->pysentimiento) (2.4.6)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->pysentimiento) (2.0.8)\n","Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->pysentimiento) (0.7.0)\n","Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->pysentimiento) (0.10.1)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->pysentimiento) (6.3.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->pysentimiento) (1.10.7)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->pysentimiento) (67.7.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->pysentimiento) (3.3.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.13.0->pysentimiento) (2022.10.31)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.13.0->pysentimiento) (0.13.3)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.13.3->pysentimiento) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.13.3->pysentimiento) (2.0.12)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.13.3->pysentimiento) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.13.3->pysentimiento) (4.0.2)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.13.3->pysentimiento) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.13.3->pysentimiento) (1.3.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.13.3->pysentimiento) (1.3.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.13.3->pysentimiento) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.13.3->pysentimiento) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.13.3->pysentimiento) (3.4)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<4.0.0,>=3.5.0->pysentimiento) (0.7.9)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<4.0.0,>=3.5.0->pysentimiento) (0.0.4)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy<4.0.0,>=3.5.0->pysentimiento) (8.1.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.0->pysentimiento) (2.1.2)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=1.13.3->pysentimiento) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=1.13.3->pysentimiento) (2022.7.1)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.0->pysentimiento) (1.3.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.13.3->pysentimiento) (1.16.0)\n","Building wheels for collected packages: emoji\n","  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171033 sha256=2fdf89650fc26e06df3337658527f0f98591d0b61823bcafe9b1556f354e1a82\n","  Stored in directory: /root/.cache/pip/wheels/31/8a/8c/315c9e5d7773f74b33d5ed33f075b49c6eaeb7cedbb86e2cf8\n","Successfully built emoji\n","Installing collected packages: emoji, pysentimiento\n","Successfully installed emoji-1.7.0 pysentimiento-0.7.0\n"]}]},{"cell_type":"markdown","source":["## hard version"],"metadata":{"id":"whtCWy3J79Ar"}},{"cell_type":"code","execution_count":11,"metadata":{"id":"V5jngBCh0qsS","colab":{"base_uri":"https://localhost:8080/","height":849},"executionInfo":{"status":"ok","timestamp":1684186670943,"user_tz":-120,"elapsed":1231073,"user":{"displayName":"Rohit Koonireddy","userId":"17353448389884117833"}},"outputId":"2040a703-ad74-4bc9-d4fa-429d3ef13670"},"outputs":[{"output_type":"stream","name":"stdout","text":["computation will run on cuda:0 now\n","train1 (17409, 2)\n","<bound method NDFrame.head of                                                    tweet  hard_label\n","0      @USER Ignora al otro, es un capullo.El problem...           1\n","1      @USER Si comicsgate se parece en algo a gamerg...           0\n","2      @USER Lee sobre Gamergate, y como eso ha cambi...           0\n","4      @USER @USER @USER Entonces como así es el merc...           1\n","5      @USER Aaah sí. Andrew Dobson. El que se dedicó...           0\n","...                                                  ...         ...\n","18260  @USER Se llama nota de corte, y es lo que dete...           0\n","18261  @USER Osea todo atack of titan parte de una ni...           0\n","18262  @USER Cuéntame más!!Es por androcentrismo? Tal...           1\n","18263   Que duro es ser tan atractiva como Jaba de Hutt.           1\n","18264  @USER A Pablo es que ya no le hacen caso en lo...           0\n","\n","[17409 rows x 2 columns]>\n","test1 (2076, 1)\n","<bound method NDFrame.head of                                                   tweet\n","0     @USER Todo gamergate desde el desarrollo hasta...\n","1     @USER @USER Hombre, no es comparable, mira lo ...\n","2     yo buscando las empresas metidas en el gamerga...\n","3     @USER Primero fue internet, luego el gamergate...\n","4     @USER Yo estuve metido en el gamergate, asi qu...\n","...                                                 ...\n","2071  @USER This straight up sounds like “you look l...\n","2072  Nathaniel is trying to help me with a new fake...\n","2073  walkin back from the gym &amp; an older lady s...\n","2074  You look like a whore of Babylon bc that’s the...\n","2075  @USER @USER You look like a whore. Stop projec...\n","\n","[2076 rows x 1 columns]>\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at sdadas/xlm-roberta-large-twitter were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at sdadas/xlm-roberta-large-twitter and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='5847' max='5847' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [5847/5847 20:04, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.485300</td>\n","      <td>0.434654</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.355900</td>\n","      <td>0.610465</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.221300</td>\n","      <td>0.827857</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Validation Accuracy: 0.8364825581395349\n","Validation F1 Score: 0.8300604229607251\n"]}],"source":["#imports\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer  \n","import os.path\n","from os import path\n","import pandas as pd\n","import numpy as np\n","import random\n","import  matplotlib. pyplot  as  plt\n","from tqdm import tqdm\n","\n","import torch\n","import torch.optim as optim\n","import  torch. nn. functional  as  F\n","\n","#set device\n","import torch\n","# device = \"cpu\"\n","device  =  torch. device('cuda:0'  if  torch. cuda. is_available() else  'cpu')\n","print(f\"computation will run on {device} now\")\n","\n","import re\n","def simple_preprocess(text):\n","  \"\"\"\n","  pass the tweet data as a series. do not use apply function\n","  only preprocesses for replacing @USER and URLS\n","  \"\"\"\n","  # print(\"i am preprocessing\")\n","  URL_RE = re.compile(r\"https?:\\/\\/[\\w\\.\\/\\?\\=\\d&#%_:/-]+\")\n","  HANDLE_RE = re.compile(r\"@\\w+\")\n","  tweets = []\n","  for t in text:\n","    t = HANDLE_RE.sub(\"@USER\", t)\n","    t = URL_RE.sub(\"HTTPURL\", t)\n","    tweets.append(t)\n","  return tweets\n","\n","\n","#instantiate label encoders\n","from sklearn.preprocessing import LabelEncoder\n","task1_encoder = LabelEncoder()\n","\n","def task1_hard_encode(df):\n","    task1_encoder.fit(all_task1_hard_labels)\n","    df['hard_label'] = task1_encoder.transform(df['hard_label'])\n","    return df\n","\n","def task1_hard_decode(df):\n","    df[\"hard_label\"] = task1_encoder.inverse_transform(df[\"hard_label\"])\n","    return df\n","\n","\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer  \n","import os.path\n","from os import path\n","\n","from transformers import XLMRobertaForSequenceClassification, XLMRobertaTokenizer, TrainingArguments, Trainer\n","from datasets import Dataset\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import accuracy_score, f1_score\n","\n","#load given data\n","import pandas as pd\n","og_train1 = pd.read_csv(\"large_train_data_task1.csv\")\n","og_dev1 = pd.read_csv(\"dev_data_task1.csv\")\n","og_test1 = pd.read_csv(\"test_final.csv\")\n","\n","all_task1_hard_labels = pd.concat([og_train1[\"hard_label\"],og_dev1[\"hard_label\"]])\n","train1_df = task1_hard_encode(og_train1)\n","\n","# print(train1_df.columns)\n","train1_df = train1_df[[\"tweet\",\"hard_label\"]].dropna()\n","train1_df = train1_df[train1_df['hard_label'] != 2]\n","train1_df[\"tweet\"] = simple_preprocess(train1_df[\"tweet\"])\n","\n","dev1_df = task1_hard_encode(og_dev1)\n","dev1_df = dev1_df[[\"tweet\",\"hard_label\"]].dropna()\n","dev1_df = dev1_df[dev1_df['hard_label'] != 2]\n","dev1_df[\"tweet\"] = simple_preprocess(dev1_df[\"tweet\"])\n","\n","test1_df = og_test1\n","test1_df = test1_df[[\"tweet\"]]\n","# test1_df = test1_df[test1_df['hard_label'] != 2]\n","test1_df[\"tweet\"] = simple_preprocess(test1_df[\"tweet\"])\n","\n","print(\"train1\",train1_df.shape)\n","print(train1_df.head)\n","print(\"test1\",test1_df.shape)\n","print(test1_df.head)\n","\n","\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","# Combine train1df and test1df into a single dataframe\n","combined_df = pd.concat([train1_df, dev1_df], ignore_index=True)\n","\n","# Shuffle the combined dataframe\n","combined_df_shuffled = combined_df.sample(frac=1, random_state=42)\n","\n","# Split the shuffled dataframe into train, validation, and test dataframes with an 80-10-10 split\n","train_df, val_df = train_test_split(combined_df_shuffled, test_size=0.15, random_state=42)\n","test_df = test1_df\n","\n","# Reset the indices of the dataframes\n","train_df = train_df.reset_index(drop=True)\n","val_df = val_df.reset_index(drop=True)\n","test_df = test_df.reset_index(drop=True)\n","\n","\n","# Load the tokenizer and model\n","model_name = \"sdadas/xlm-roberta-large-twitter\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2,ignore_mismatched_sizes=True)\n","\n","# Set parameters\n","MAX_LENGTH = 128\n","\n","# Define the training arguments\n","training_args = TrainingArguments(\n","    # accelerator = Accelerator(),\n","    output_dir=\"./output\",\n","    num_train_epochs=3,\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=16,\n","    learning_rate=2e-05, #replaced with optuna best, original is 3.395067563962819e-05\n","    weight_decay=0.004891290652279793, #replaced with opunta best, original is 0.004891290652279793\n","    logging_dir=\"./logs\",\n","    # metric_for_best_model=\"f1_score\",\n","    # greater_is_better=True,\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    logging_strategy=\"epoch\"\n",")\n","\n","def convert_to_dataset(df):\n","    df = {\"text\": df['tweet'].tolist(), \"label\": df[\"hard_label\"].tolist()}\n","    dataset = Dataset.from_dict(df)\n","    return dataset\n","\n","def convert_to_dataset_test(df):\n","    df = {\"text\": df['tweet'].tolist()}\n","    dataset = Dataset.from_dict(df)\n","    return dataset\n","\n","# Convert dataframe to dataset\n","train_dataset = convert_to_dataset(train_df)\n","val_dataset = convert_to_dataset(val_df)\n","test_dataset = convert_to_dataset_test(test_df)\n","\n","\n","# Create the datasets\n","train_encodings = tokenizer(train_dataset[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\n","train_dataset = Dataset.from_dict({\"input_ids\": train_encodings[\"input_ids\"], \"attention_mask\": train_encodings[\"attention_mask\"], \"label\": train_dataset[\"label\"]})\n","\n","val_encodings = tokenizer(val_dataset[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\n","val_dataset = Dataset.from_dict({\"input_ids\": val_encodings[\"input_ids\"], \"attention_mask\": val_encodings[\"attention_mask\"], \"label\": val_dataset[\"label\"]})\n","\n","\n","test_encodings = tokenizer(test_dataset[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\n","test_dataset = Dataset.from_dict({\"input_ids\": test_encodings[\"input_ids\"], \"attention_mask\": test_encodings[\"attention_mask\"]})\n","\n","\n","def compute_f1_score(pred):\n","    # pred is a tuple (predictions, labels)\n","    predictions, labels = pred\n","    # Compute the F1 score\n","    f1 = f1_score(labels, predictions.argmax(axis=1), average='macro')\n","    return {\"f1_score\": f1}\n","\n","# Define the trainer for each fold\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset\n",")\n","\n","# Train the model for each fold\n","trainer.train()\n","\n","from scipy.special import softmax\n","from sklearn.metrics import accuracy_score, f1_score\n","# Get predictions on the validation set\n","val_predictions = trainer.predict(val_dataset)\n","val_pred_labels = np.argmax(val_predictions.predictions, axis=1)\n","val_true_labels = val_dataset[\"label\"]\n","\n","val_accuracy = accuracy_score(val_true_labels, val_pred_labels)\n","val_f1_score = f1_score(val_true_labels, val_pred_labels)\n","\n","print(\"Validation Accuracy:\", val_accuracy)\n","print(\"Validation F1 Score:\", val_f1_score)"]},{"cell_type":"code","source":["# Get predictions on the test set\n","from scipy.special import softmax\n","test_predictions = trainer.predict(test_dataset)\n","test_pred_labels = np.argmax(test_predictions.predictions, axis=1)\n","test_pred_probs = softmax(test_predictions.predictions).tolist()\n","\n","#get probabilities to a list\n","test_probs_list = [] \n","for logits in test_pred_probs:\n","    test_probs_list.append({'YES': logits[0], 'NO': logits[1]})"],"metadata":{"id":"J1lS4ebph3UX","colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"status":"ok","timestamp":1684186682718,"user_tz":-120,"elapsed":11789,"user":{"displayName":"Rohit Koonireddy","userId":"17353448389884117833"}},"outputId":"10e8f80f-f920-4e83-8beb-1cd9e102bc3f"},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}}]},{"cell_type":"code","source":["import json\n","def get_json_for_evaluation(df, probs_list, ids, gold):\n","    gold[\"index\"] = gold[\"id_EXIST\"].astype(int)\n","    output_dict = {}\n","    decoded_labels = task1_hard_decode(df)[\"hard_label\"].tolist()\n","    print(\"decoded labels length:\",len(decoded_labels))\n","    for i, row in enumerate(probs_list):\n","        soft_label = row\n","        hard_label = decoded_labels[i]\n","        local_dict = {\"hard_label\":hard_label,\"soft_label\": soft_label}\n","        output_dict[int(gold[\"index\"][i])] = local_dict\n","    \n","    filename = \"/content/drive/MyDrive/ml4nlp2/for_submission/group-submissions/exist2023_roh-neil/task1_roh-neil_3.json\"\n","    print(\"output generated as json\")\n","    with open(filename, 'w') as file:\n","        json.dump(output_dict, file, indent=4)\n","    return output_dict\n","\n","outputs_json = get_json_for_evaluation(pd.DataFrame(test_pred_labels,columns=[\"hard_label\"]), test_probs_list, og_test1[[\"id_EXIST\"]],og_test1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8W5BEvfehB3F","executionInfo":{"status":"ok","timestamp":1684186682719,"user_tz":-120,"elapsed":19,"user":{"displayName":"Rohit Koonireddy","userId":"17353448389884117833"}},"outputId":"006a517e-7517-4325-efb9-94b9ba64fb33"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["decoded labels length: 2076\n","output generated as json\n"]}]},{"cell_type":"markdown","source":["## soft version"],"metadata":{"id":"2CZY2hTmfvBH"}},{"cell_type":"code","source":["#imports\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer  \n","import os.path\n","from os import path\n","import pandas as pd\n","import numpy as np\n","import random\n","import  matplotlib. pyplot  as  plt\n","from tqdm import tqdm\n","\n","import torch\n","import torch.optim as optim\n","import  torch. nn. functional  as  F\n","\n","#set device\n","import torch\n","# device = \"cpu\"\n","device  =  torch. device('cuda:0'  if  torch. cuda. is_available() else  'cpu')\n","print(f\"computation will run on {device} now\")\n","\n","import re\n","def simple_preprocess(text):\n","  \"\"\"\n","  pass the tweet data as a series. do not use apply function\n","  only preprocesses for replacing @USER and URLS\n","  \"\"\"\n","  # print(\"i am preprocessing\")\n","  URL_RE = re.compile(r\"https?:\\/\\/[\\w\\.\\/\\?\\=\\d&#%_:/-]+\")\n","  HANDLE_RE = re.compile(r\"@\\w+\")\n","  tweets = []\n","  for t in text:\n","    t = HANDLE_RE.sub(\"@USER\", t)\n","    t = URL_RE.sub(\"HTTPURL\", t)\n","    tweets.append(t)\n","  return tweets\n","\n","\n","#instantiate label encoders\n","def convert_logits_to_list(logits_dict):\n","    logits_dict = eval(logits_dict)\n","    logits_list = [logits_dict[\"YES\"], logits_dict[\"NO\"]]\n","    return logits_list\n","\n","def convert_list_to_logits(logits_list):\n","    logits_dict = {\"YES\": logits_list[0], \"NO\": logits_list[1]}\n","    return logits_dict\n","\n","def check_dtype(given_data):\n","    return eval(given_data)\n","\n","\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer  \n","import os.path\n","from os import path\n","\n","from transformers import XLMRobertaForSequenceClassification, XLMRobertaTokenizer, TrainingArguments, Trainer\n","from datasets import Dataset\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import accuracy_score, f1_score\n","\n","\n","\n","#load given data\n","import pandas as pd\n","og_train1 = pd.read_csv(\"train_data_task1.csv\")\n","og_dev1 = pd.read_csv(\"dev_data_task1.csv\")\n","og_test1 = pd.read_csv(\"test_final.csv\")\n","\n","og_train1[\"soft_label\"] = og_train1[\"soft_label\"].apply(convert_logits_to_list)\n","og_train1[\"tweet\"] = simple_preprocess(og_train1[\"tweet\"])\n","# og_train1[\"soft_label\"] = og_train1[\"soft_label\"].apply(check_dtype) \n","train1_df = og_train1[[\"tweet\",\"soft_label\"]].dropna()\n","\n","\n","og_dev1[\"soft_label\"] = og_dev1[\"soft_label\"].apply(convert_logits_to_list)\n","og_dev1[\"tweet\"] = simple_preprocess(og_dev1[\"tweet\"])\n","# og_dev1[\"soft_label\"] = og_dev1[\"soft_label\"].apply(check_dtype) \n","dev1_df = og_dev1[[\"tweet\",\"soft_label\"]].dropna()\n","\n","test1_df = og_test1\n","test1_df = test1_df[[\"tweet\"]]\n","# test1_df = test1_df[test1_df['hard_label'] != 2]\n","test1_df[\"tweet\"] = simple_preprocess(test1_df[\"tweet\"])\n","\n","print(\"train1\",train1_df.shape)\n","print(train1_df.head)\n","print(\"test1\",test1_df.shape)\n","print(test1_df.head)\n","\n","\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","# Combine train1df and test1df into a single dataframe\n","combined_df = pd.concat([train1_df, dev1_df], ignore_index=True)\n","\n","# Shuffle the combined dataframe\n","combined_df_shuffled = combined_df.sample(frac=1, random_state=42)\n","\n","# Split the shuffled dataframe into train, validation, and test dataframes with an 80-10-10 split\n","train_df, val_df = train_test_split(combined_df_shuffled, test_size=0.15, random_state=42)\n","test_df = test1_df\n","\n","# Reset the indices of the dataframes\n","train_df = train_df.reset_index(drop=True)\n","val_df = val_df.reset_index(drop=True)\n","test_df = test_df.reset_index(drop=True)\n","\n","\n","# Load the tokenizer and model\n","model_name = \"sdadas/xlm-roberta-large-twitter\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2,ignore_mismatched_sizes=True)\n","\n","\n","# Set parameters\n","MAX_LENGTH = 128\n","\n","# Define the training arguments\n","training_args = TrainingArguments(\n","    # accelerator = Accelerator(),\n","    output_dir=\"./output\",\n","    num_train_epochs=3,\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=16,\n","    learning_rate = 2.5174100002380197e-05, #replaced with optuna best, original is 3.395067563962819e-05, 2.5174100002380197e-05\n","    weight_decay = 0.009582405316762398, #replaced with opunta best, original is 0.004891290652279793\n","    logging_dir=\"./logs\",\n","    # metric_for_best_model=\"f1_score\",\n","    # greater_is_better=True,\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    logging_strategy=\"epoch\"\n",")\n","\n","import torch.nn as nn\n","\n","def convert_to_dataset(df):\n","    df = {\"text\": df['tweet'].tolist(), \"label\": df[\"soft_label\"].tolist()}\n","    dataset = Dataset.from_dict(df)\n","    return dataset\n","\n","def convert_to_dataset_test(df):\n","    df = {\"text\": df['tweet'].tolist()}\n","    dataset = Dataset.from_dict(df)\n","    return dataset\n","\n","# Convert dataframe to dataset\n","train_dataset = convert_to_dataset(train_df)\n","val_dataset = convert_to_dataset(val_df)\n","test_dataset = convert_to_dataset_test(test_df)\n","\n","\n","# Create the datasets\n","train_encodings = tokenizer(train_dataset[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\n","train_dataset = Dataset.from_dict({\"input_ids\": train_encodings[\"input_ids\"], \"attention_mask\": train_encodings[\"attention_mask\"], \"label\": train_dataset[\"label\"]})\n","\n","val_encodings = tokenizer(val_dataset[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\n","val_dataset = Dataset.from_dict({\"input_ids\": val_encodings[\"input_ids\"], \"attention_mask\": val_encodings[\"attention_mask\"], \"label\": val_dataset[\"label\"]})\n","\n","\n","test_encodings = tokenizer(test_dataset[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\n","test_dataset = Dataset.from_dict({\"input_ids\": test_encodings[\"input_ids\"], \"attention_mask\": test_encodings[\"attention_mask\"]})\n","\n","def custom_loss_fn(logits, soft_labels):\n","    probs = F.softmax(logits, dim=1)\n","    # Apply nn.CrossEntropyLoss\n","    loss = nn.CrossEntropyLoss(reduction=\"sum\",label_smoothing=0.15)(probs, soft_labels)\n","    return loss\n","\n","class CustomTrainer(Trainer):\n","    def compute_loss(self, model, inputs, return_outputs=False):\n","        labels = inputs.pop(\"labels\")\n","        outputs = model(**inputs)\n","        logits = outputs.logits\n","        loss = custom_loss_fn(logits, labels)\n","        return (loss, outputs) if return_outputs else loss\n","\n","\n","# Define the trainer for each fold\n","trainer = CustomTrainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset\n",")\n","\n","# Train the model for each fold\n","trainer.train()\n","\n","from scipy.special import softmax\n","from sklearn.metrics import accuracy_score, f1_score\n","# Get predictions on the validation set\n","val_predictions = trainer.predict(val_dataset)\n","val_pred_logit_labels = val_predictions.predictions\n","val_pred_probabilities = softmax(val_pred_logit_labels, axis=1)\n","\n","val_pred_hard_labels = np.argmax(val_pred_probabilities, axis=1)\n","val_true_labels = val_dataset[\"label\"]\n","val_true_hard_labels = np.argmax(val_dataset[\"label\"],axis=1)\n","\n","# Calculate evaluation metrics for each fold\n","val_accuracy = accuracy_score(val_true_hard_labels, val_pred_hard_labels)\n","val_f1_score = f1_score(val_true_hard_labels, val_pred_hard_labels,average='weighted')\n","\n","print(\"Validation Accuracy:\", val_accuracy)\n","print(\"Validation F1 Score:\", val_f1_score)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"6SEv2DpH5-mC","executionInfo":{"status":"ok","timestamp":1684185352863,"user_tz":-120,"elapsed":574978,"user":{"displayName":"Rohit Koonireddy","userId":"17353448389884117833"}},"outputId":"f9b1b503-1969-4c47-84e7-b037dd2e2766"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["computation will run on cuda:0 now\n","train1 (6920, 2)\n","<bound method NDFrame.head of                                                   tweet  \\\n","0     @USER Ignora al otro, es un capullo.El problem...   \n","1     @USER Si comicsgate se parece en algo a gamerg...   \n","2     @USER Lee sobre Gamergate, y como eso ha cambi...   \n","3     @USER Un retraso social bastante lamentable, g...   \n","4     @USER @USER @USER Entonces como así es el merc...   \n","...                                                 ...   \n","6915  idk why y’all bitches think having half your a...   \n","6916  This has been a part of an experiment with @US...   \n","6917  \"Take me already\" \"Not yet. You gotta be ready...   \n","6918    @USER why do you look like a whore? /lh HTTPURL   \n","6919  ik when mandy says “you look like a whore” i l...   \n","\n","                                     soft_label  \n","0      [0.833333333333333, 0.16666666666666602]  \n","1      [0.16666666666666602, 0.833333333333333]  \n","2                                    [0.0, 1.0]  \n","3                                    [0.5, 0.5]  \n","4     [0.6666666666666661, 0.33333333333333304]  \n","...                                         ...  \n","6915                                 [1.0, 0.0]  \n","6916                                 [1.0, 0.0]  \n","6917  [0.6666666666666661, 0.33333333333333304]  \n","6918                                 [1.0, 0.0]  \n","6919   [0.833333333333333, 0.16666666666666602]  \n","\n","[6920 rows x 2 columns]>\n","test1 (2076, 1)\n","<bound method NDFrame.head of                                                   tweet\n","0     @USER Todo gamergate desde el desarrollo hasta...\n","1     @USER @USER Hombre, no es comparable, mira lo ...\n","2     yo buscando las empresas metidas en el gamerga...\n","3     @USER Primero fue internet, luego el gamergate...\n","4     @USER Yo estuve metido en el gamergate, asi qu...\n","...                                                 ...\n","2071  @USER This straight up sounds like “you look l...\n","2072  Nathaniel is trying to help me with a new fake...\n","2073  walkin back from the gym &amp; an older lady s...\n","2074  You look like a whore of Babylon bc that’s the...\n","2075  @USER @USER You look like a whore. Stop projec...\n","\n","[2076 rows x 1 columns]>\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at sdadas/xlm-roberta-large-twitter were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at sdadas/xlm-roberta-large-twitter and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='2538' max='2538' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2538/2538 09:18, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>4.892300</td>\n","      <td>9.417205</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>4.591900</td>\n","      <td>9.296276</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>4.451500</td>\n","      <td>9.299561</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Validation Accuracy: 0.8333333333333334\n","Validation F1 Score: 0.8333011698389229\n"]}]},{"cell_type":"code","source":["# Get predictions on the test set\n","from scipy.special import softmax\n","test_predictions1 = trainer.predict(test_dataset)\n","test_pred_labels1 = np.argmax(test_predictions1.predictions, axis=1)\n","test_pred_probs1 = softmax(test_predictions1.predictions).tolist()\n","\n","#get probabilities to a list\n","test_probs_list1 = [] \n","for logits in test_pred_probs1:\n","    test_probs_list1.append({'YES': logits[0], 'NO': logits[1]})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"DS0pmqKaOR-W","executionInfo":{"status":"ok","timestamp":1684185375474,"user_tz":-120,"elapsed":12294,"user":{"displayName":"Rohit Koonireddy","userId":"17353448389884117833"}},"outputId":"4e792a2c-4eae-4c8a-b559-9fa812154f37"},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}}]},{"cell_type":"code","source":["import json\n","def get_json_for_evaluation(df, probs_list, ids, gold):\n","    gold[\"index\"] = gold[\"id_EXIST\"].astype(int)\n","    output_dict = {}\n","    decoded_labels = task1_hard_decode(df)[\"hard_label\"].tolist()\n","    print(\"decoded labels length:\",len(decoded_labels))\n","    for i, row in enumerate(probs_list):\n","        soft_label = row\n","        hard_label = decoded_labels[i]\n","        local_dict = {\"hard_label\":hard_label,\"soft_label\": soft_label}\n","        output_dict[int(gold[\"index\"][i])] = local_dict\n","    \n","    filename = \"/content/drive/MyDrive/ml4nlp2/for_submission/group-submissions/exist2023_roh-neil/task1_roh-neil_2.json\"\n","    print(\"output generated as json\")\n","    with open(filename, 'w') as file:\n","        json.dump(output_dict, file, indent=4)\n","    return output_dict\n","\n","outputs_json = get_json_for_evaluation(pd.DataFrame(test_pred_labels,columns=[\"hard_label\"]), test_probs_list1, og_test1[[\"id_EXIST\"]],og_test1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EXgXIALHPKtP","executionInfo":{"status":"ok","timestamp":1684185381814,"user_tz":-120,"elapsed":371,"user":{"displayName":"Rohit Koonireddy","userId":"17353448389884117833"}},"outputId":"81711040-dcd0-455e-ad0b-651161b6c950"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["decoded labels length: 2076\n","output generated as json\n"]}]}]}